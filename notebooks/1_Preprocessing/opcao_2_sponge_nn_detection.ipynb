{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção de Fósseis com Redes Neurais\n",
    "\n",
    "Pipeline para detecção automática de esponjas Leptomitid usando:\n",
    "\n",
    "1. **SAM (Segment Anything)** - Segmentação zero-shot\n",
    "2. **Grounding DINO + SAM** - Detecção open-vocabulary\n",
    "3. **Claude Vision API** - Detecção via LLM multimodal\n",
    "4. **YOLOv8** - Fine-tuning com poucas amostras\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependências\n",
    "# Descomente conforme necessário\n",
    "\n",
    "# SAM\n",
    "# !pip install segment-anything\n",
    "# !pip install opencv-python matplotlib\n",
    "\n",
    "# Grounding DINO + SAM (via groundingdino)\n",
    "# !pip install groundingdino-py\n",
    "\n",
    "# YOLOv8\n",
    "# !pip install ultralytics\n",
    "\n",
    "# Para Claude API\n",
    "# !pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Detection:\n",
    "    \"\"\"Detecção de fóssil\"\"\"\n",
    "    bbox: Tuple[int, int, int, int]  # x1, y1, x2, y2\n",
    "    mask: Optional[np.ndarray]\n",
    "    confidence: float\n",
    "    label: str = \"fossil_sponge\"\n",
    "\n",
    "\n",
    "def load_image(path: str) -> np.ndarray:\n",
    "    img = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def visualize_detections(img: np.ndarray, detections: List[Detection], title: str = \"\"):\n",
    "    \"\"\"Visualiza detecções na imagem\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Original com bboxes\n",
    "    vis = img.copy()\n",
    "    for i, det in enumerate(detections):\n",
    "        x1, y1, x2, y2 = det.bbox\n",
    "        cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        cv2.putText(vis, f\"#{i} {det.confidence:.2f}\", (x1, y1-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    axes[0].imshow(vis)\n",
    "    axes[0].set_title(f\"{title} - Bounding Boxes\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Máscaras combinadas\n",
    "    if any(det.mask is not None for det in detections):\n",
    "        overlay = img.copy().astype(float)\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(detections)))\n",
    "        \n",
    "        for det, color in zip(detections, colors):\n",
    "            if det.mask is not None:\n",
    "                mask_3d = np.stack([det.mask]*3, axis=-1)\n",
    "                overlay = np.where(mask_3d, overlay * 0.5 + np.array(color[:3]) * 255 * 0.5, overlay)\n",
    "        \n",
    "        axes[1].imshow(overlay.astype(np.uint8))\n",
    "        axes[1].set_title(f\"{title} - Segmentation Masks\")\n",
    "    else:\n",
    "        axes[1].imshow(vis)\n",
    "        axes[1].set_title(\"No masks available\")\n",
    "    \n",
    "    axes[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. SAM (Segment Anything Model)\n",
    "\n",
    "Meta's SAM é um modelo de segmentação zero-shot. Funciona com prompts:\n",
    "- **Pontos**: Clique no objeto\n",
    "- **Bounding Box**: Caixa aproximada\n",
    "- **Automático**: Segmenta tudo na imagem\n",
    "\n",
    "### Download do modelo:\n",
    "```bash\n",
    "# SAM ViT-H (2.4GB) - mais preciso\n",
    "wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "\n",
    "# SAM ViT-B (375MB) - mais rápido\n",
    "wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDetector:\n",
    "    \"\"\"\n",
    "    Detector usando Segment Anything Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 checkpoint_path: str = \"sam_vit_b_01ec64.pth\",\n",
    "                 model_type: str = \"vit_b\",\n",
    "                 device: str = DEVICE):\n",
    "        \n",
    "        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "        \n",
    "        print(f\"Carregando SAM ({model_type})...\")\n",
    "        self.sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "        self.sam.to(device)\n",
    "        \n",
    "        self.predictor = SamPredictor(self.sam)\n",
    "        self.auto_generator = SamAutomaticMaskGenerator(\n",
    "            self.sam,\n",
    "            points_per_side=32,\n",
    "            pred_iou_thresh=0.86,\n",
    "            stability_score_thresh=0.92,\n",
    "            min_mask_region_area=10000,  # Filtrar regiões pequenas\n",
    "        )\n",
    "        \n",
    "        print(\"SAM carregado!\")\n",
    "    \n",
    "    def segment_with_points(self, \n",
    "                            img: np.ndarray,\n",
    "                            points: List[Tuple[int, int]],\n",
    "                            labels: List[int] = None) -> List[Detection]:\n",
    "        \"\"\"\n",
    "        Segmenta usando pontos como prompt.\n",
    "        \n",
    "        Args:\n",
    "            img: Imagem RGB\n",
    "            points: Lista de (x, y) indicando objetos\n",
    "            labels: 1 para foreground, 0 para background\n",
    "        \"\"\"\n",
    "        self.predictor.set_image(img)\n",
    "        \n",
    "        points_np = np.array(points)\n",
    "        labels_np = np.array(labels) if labels else np.ones(len(points))\n",
    "        \n",
    "        masks, scores, _ = self.predictor.predict(\n",
    "            point_coords=points_np,\n",
    "            point_labels=labels_np,\n",
    "            multimask_output=True\n",
    "        )\n",
    "        \n",
    "        # Pegar a melhor máscara\n",
    "        best_idx = np.argmax(scores)\n",
    "        best_mask = masks[best_idx]\n",
    "        \n",
    "        # Calcular bbox\n",
    "        coords = np.where(best_mask)\n",
    "        if len(coords[0]) > 0:\n",
    "            y1, y2 = coords[0].min(), coords[0].max()\n",
    "            x1, x2 = coords[1].min(), coords[1].max()\n",
    "            bbox = (x1, y1, x2, y2)\n",
    "        else:\n",
    "            bbox = (0, 0, 0, 0)\n",
    "        \n",
    "        return [Detection(\n",
    "            bbox=bbox,\n",
    "            mask=best_mask,\n",
    "            confidence=float(scores[best_idx])\n",
    "        )]\n",
    "    \n",
    "    def segment_with_box(self, \n",
    "                         img: np.ndarray,\n",
    "                         box: Tuple[int, int, int, int]) -> List[Detection]:\n",
    "        \"\"\"\n",
    "        Segmenta usando bounding box como prompt.\n",
    "        \n",
    "        Args:\n",
    "            img: Imagem RGB\n",
    "            box: (x1, y1, x2, y2) aproximado do objeto\n",
    "        \"\"\"\n",
    "        self.predictor.set_image(img)\n",
    "        \n",
    "        masks, scores, _ = self.predictor.predict(\n",
    "            box=np.array(box),\n",
    "            multimask_output=True\n",
    "        )\n",
    "        \n",
    "        best_idx = np.argmax(scores)\n",
    "        best_mask = masks[best_idx]\n",
    "        \n",
    "        # Refinar bbox baseado na máscara\n",
    "        coords = np.where(best_mask)\n",
    "        if len(coords[0]) > 0:\n",
    "            y1, y2 = coords[0].min(), coords[0].max()\n",
    "            x1, x2 = coords[1].min(), coords[1].max()\n",
    "            refined_bbox = (x1, y1, x2, y2)\n",
    "        else:\n",
    "            refined_bbox = box\n",
    "        \n",
    "        return [Detection(\n",
    "            bbox=refined_bbox,\n",
    "            mask=best_mask,\n",
    "            confidence=float(scores[best_idx])\n",
    "        )]\n",
    "    \n",
    "    def segment_automatic(self, \n",
    "                          img: np.ndarray,\n",
    "                          min_area: int = 10000,\n",
    "                          max_area: int = None,\n",
    "                          aspect_ratio_range: Tuple[float, float] = (1.5, 10.0)) -> List[Detection]:\n",
    "        \"\"\"\n",
    "        Segmentação automática com filtros para fósseis.\n",
    "        \n",
    "        Args:\n",
    "            img: Imagem RGB\n",
    "            min_area: Área mínima em pixels\n",
    "            max_area: Área máxima (None = 50% da imagem)\n",
    "            aspect_ratio_range: Fósseis Leptomitid são alongados\n",
    "        \"\"\"\n",
    "        if max_area is None:\n",
    "            max_area = img.shape[0] * img.shape[1] * 0.5\n",
    "        \n",
    "        print(\"Gerando máscaras automáticas...\")\n",
    "        masks_data = self.auto_generator.generate(img)\n",
    "        print(f\"  {len(masks_data)} máscaras geradas\")\n",
    "        \n",
    "        detections = []\n",
    "        \n",
    "        for mask_info in masks_data:\n",
    "            mask = mask_info['segmentation']\n",
    "            area = mask_info['area']\n",
    "            bbox = mask_info['bbox']  # x, y, w, h formato COCO\n",
    "            score = mask_info['predicted_iou']\n",
    "            \n",
    "            # Filtrar por área\n",
    "            if area < min_area or area > max_area:\n",
    "                continue\n",
    "            \n",
    "            # Filtrar por aspect ratio (fósseis são alongados)\n",
    "            x, y, w, h = bbox\n",
    "            aspect_ratio = max(w, h) / (min(w, h) + 1e-6)\n",
    "            \n",
    "            if aspect_ratio < aspect_ratio_range[0] or aspect_ratio > aspect_ratio_range[1]:\n",
    "                continue\n",
    "            \n",
    "            # Converter bbox para x1,y1,x2,y2\n",
    "            bbox_xyxy = (x, y, x + w, y + h)\n",
    "            \n",
    "            detections.append(Detection(\n",
    "                bbox=bbox_xyxy,\n",
    "                mask=mask,\n",
    "                confidence=score\n",
    "            ))\n",
    "        \n",
    "        # Ordenar por confiança\n",
    "        detections.sort(key=lambda d: d.confidence, reverse=True)\n",
    "        \n",
    "        print(f\"  {len(detections)} detecções após filtros\")\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Exemplo SAM\n",
    "# ============================================\n",
    "\n",
    "# sam_detector = SAMDetector(\n",
    "#     checkpoint_path=\"./models/sam_vit_b_01ec64.pth\",\n",
    "#     model_type=\"vit_b\"\n",
    "# )\n",
    "\n",
    "# img = load_image(\"./sponge_images/GM1405_2.JPG\")\n",
    "\n",
    "# # Opção 1: Com ponto no centro do fóssil\n",
    "# detections = sam_detector.segment_with_points(img, [(600, 400)], [1])\n",
    "\n",
    "# # Opção 2: Com box aproximado\n",
    "# detections = sam_detector.segment_with_box(img, (200, 300, 1000, 500))\n",
    "\n",
    "# # Opção 3: Automático\n",
    "# detections = sam_detector.segment_automatic(img)\n",
    "\n",
    "# visualize_detections(img, detections, \"SAM Detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Grounding DINO + SAM\n",
    "\n",
    "Grounding DINO detecta objetos por descrição textual. Combinado com SAM, dá detecção + segmentação.\n",
    "\n",
    "### Setup:\n",
    "```bash\n",
    "pip install groundingdino-py\n",
    "# ou\n",
    "git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
    "cd GroundingDINO && pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundedSAMDetector:\n",
    "    \"\"\"\n",
    "    Grounding DINO para detecção + SAM para segmentação\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 grounding_config: str = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\",\n",
    "                 grounding_checkpoint: str = \"groundingdino_swint_ogc.pth\",\n",
    "                 sam_checkpoint: str = \"sam_vit_b_01ec64.pth\",\n",
    "                 sam_type: str = \"vit_b\",\n",
    "                 device: str = DEVICE):\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        # Carregar Grounding DINO\n",
    "        try:\n",
    "            from groundingdino.util.inference import load_model, predict\n",
    "            from groundingdino.util import box_ops\n",
    "            \n",
    "            print(\"Carregando Grounding DINO...\")\n",
    "            self.grounding_model = load_model(grounding_config, grounding_checkpoint, device=device)\n",
    "            self.grounding_predict = predict\n",
    "            self.box_ops = box_ops\n",
    "            self.grounding_available = True\n",
    "            print(\"Grounding DINO carregado!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Grounding DINO não disponível: {e}\")\n",
    "            self.grounding_available = False\n",
    "        \n",
    "        # Carregar SAM\n",
    "        try:\n",
    "            from segment_anything import sam_model_registry, SamPredictor\n",
    "            \n",
    "            print(\"Carregando SAM...\")\n",
    "            sam = sam_model_registry[sam_type](checkpoint=sam_checkpoint)\n",
    "            sam.to(device)\n",
    "            self.sam_predictor = SamPredictor(sam)\n",
    "            self.sam_available = True\n",
    "            print(\"SAM carregado!\")\n",
    "        except Exception as e:\n",
    "            print(f\"SAM não disponível: {e}\")\n",
    "            self.sam_available = False\n",
    "    \n",
    "    def detect(self,\n",
    "               img: np.ndarray,\n",
    "               text_prompt: str = \"fossil . sponge . elongated organism . paleontological specimen\",\n",
    "               box_threshold: float = 0.25,\n",
    "               text_threshold: float = 0.25) -> List[Detection]:\n",
    "        \"\"\"\n",
    "        Detecta objetos baseado em descrição textual.\n",
    "        \n",
    "        Args:\n",
    "            img: Imagem RGB\n",
    "            text_prompt: Descrições separadas por \" . \"\n",
    "            box_threshold: Threshold para detecção\n",
    "            text_threshold: Threshold para matching de texto\n",
    "        \"\"\"\n",
    "        if not self.grounding_available:\n",
    "            print(\"Grounding DINO não disponível\")\n",
    "            return []\n",
    "        \n",
    "        from groundingdino.util.inference import load_image as gd_load_image\n",
    "        import torchvision.transforms as T\n",
    "        \n",
    "        # Preparar imagem para Grounding DINO\n",
    "        transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        img_pil = Image.fromarray(img)\n",
    "        img_transformed = transform(img_pil)\n",
    "        \n",
    "        # Detectar\n",
    "        boxes, logits, phrases = self.grounding_predict(\n",
    "            self.grounding_model,\n",
    "            img_transformed,\n",
    "            text_prompt,\n",
    "            box_threshold,\n",
    "            text_threshold,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        detections = []\n",
    "        \n",
    "        for box, score, phrase in zip(boxes, logits, phrases):\n",
    "            # Converter de cxcywh normalizado para xyxy\n",
    "            cx, cy, bw, bh = box.cpu().numpy()\n",
    "            x1 = int((cx - bw/2) * w)\n",
    "            y1 = int((cy - bh/2) * h)\n",
    "            x2 = int((cx + bw/2) * w)\n",
    "            y2 = int((cy + bh/2) * h)\n",
    "            \n",
    "            # Segmentar com SAM se disponível\n",
    "            mask = None\n",
    "            if self.sam_available:\n",
    "                self.sam_predictor.set_image(img)\n",
    "                masks, scores, _ = self.sam_predictor.predict(\n",
    "                    box=np.array([x1, y1, x2, y2]),\n",
    "                    multimask_output=False\n",
    "                )\n",
    "                mask = masks[0]\n",
    "            \n",
    "            detections.append(Detection(\n",
    "                bbox=(x1, y1, x2, y2),\n",
    "                mask=mask,\n",
    "                confidence=float(score),\n",
    "                label=phrase\n",
    "            ))\n",
    "        \n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Exemplo Grounding DINO + SAM\n",
    "# ============================================\n",
    "\n",
    "# detector = GroundedSAMDetector(\n",
    "#     grounding_checkpoint=\"./models/groundingdino_swint_ogc.pth\",\n",
    "#     sam_checkpoint=\"./models/sam_vit_b_01ec64.pth\"\n",
    "# )\n",
    "\n",
    "# img = load_image(\"./sponge_images/GM1405_2.JPG\")\n",
    "\n",
    "# # Prompts variados para fósseis\n",
    "# detections = detector.detect(\n",
    "#     img,\n",
    "#     text_prompt=\"fossil . orange elongated shape . preserved organism . sponge specimen\",\n",
    "#     box_threshold=0.2,\n",
    "#     text_threshold=0.2\n",
    "# )\n",
    "\n",
    "# visualize_detections(img, detections, \"Grounding DINO + SAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Claude Vision API\n",
    "\n",
    "Usar Claude para detectar bounding boxes via prompt. Simples e não requer GPU local.\n",
    "\n",
    "**Vantagens:**\n",
    "- Não precisa de setup de modelos locais\n",
    "- Entende contexto (\"esponjas fósseis do Cambriano\")\n",
    "- Pode dar informações adicionais sobre o espécime\n",
    "\n",
    "**Desvantagens:**\n",
    "- Custo de API\n",
    "- Latência\n",
    "- Não gera máscaras de segmentação (apenas bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import re\n",
    "\n",
    "class ClaudeVisionDetector:\n",
    "    \"\"\"\n",
    "    Detector usando Claude Vision API\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        try:\n",
    "            import anthropic\n",
    "            self.client = anthropic.Anthropic(api_key=api_key)\n",
    "            self.available = True\n",
    "        except Exception as e:\n",
    "            print(f\"Anthropic client não disponível: {e}\")\n",
    "            self.available = False\n",
    "    \n",
    "    def _image_to_base64(self, img: np.ndarray) -> str:\n",
    "        \"\"\"Converte imagem para base64\"\"\"\n",
    "        # Redimensionar se muito grande (limite de 20MB)\n",
    "        h, w = img.shape[:2]\n",
    "        max_dim = 2000\n",
    "        if max(h, w) > max_dim:\n",
    "            scale = max_dim / max(h, w)\n",
    "            img = cv2.resize(img, None, fx=scale, fy=scale)\n",
    "        \n",
    "        _, buffer = cv2.imencode('.jpg', cv2.cvtColor(img, cv2.COLOR_RGB2BGR), \n",
    "                                  [cv2.IMWRITE_JPEG_QUALITY, 85])\n",
    "        return base64.b64encode(buffer).decode('utf-8')\n",
    "    \n",
    "    def detect(self, \n",
    "               img: np.ndarray,\n",
    "               context: str = \"Cambrian fossil sponges (Leptomitid)\") -> List[Detection]:\n",
    "        \"\"\"\n",
    "        Detecta fósseis usando Claude Vision.\n",
    "        \n",
    "        Args:\n",
    "            img: Imagem RGB\n",
    "            context: Contexto sobre o tipo de fóssil\n",
    "        \"\"\"\n",
    "        if not self.available:\n",
    "            print(\"Claude client não disponível\")\n",
    "            return []\n",
    "        \n",
    "        h, w = img.shape[:2]\n",
    "        img_b64 = self._image_to_base64(img)\n",
    "        \n",
    "        prompt = f\"\"\"Analyze this paleontological photograph showing {context}.\n",
    "\n",
    "Detect all fossil specimens in the image. For each fossil found, provide:\n",
    "1. Bounding box coordinates as [x1, y1, x2, y2] in pixels\n",
    "2. Confidence score (0-1)\n",
    "3. Brief description\n",
    "\n",
    "Image dimensions: {w} x {h} pixels\n",
    "\n",
    "Respond ONLY with a JSON array in this exact format:\n",
    "[\n",
    "  {{\n",
    "    \"bbox\": [x1, y1, x2, y2],\n",
    "    \"confidence\": 0.95,\n",
    "    \"description\": \"elongated sponge with visible spicules\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "If no fossils are found, return an empty array: []\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=\"claude-sonnet-4-20250514\",\n",
    "                max_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"image\",\n",
    "                                \"source\": {\n",
    "                                    \"type\": \"base64\",\n",
    "                                    \"media_type\": \"image/jpeg\",\n",
    "                                    \"data\": img_b64\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Parsear resposta\n",
    "            text = response.content[0].text\n",
    "            \n",
    "            # Extrair JSON\n",
    "            json_match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "            if json_match:\n",
    "                results = json.loads(json_match.group())\n",
    "            else:\n",
    "                print(f\"Não foi possível parsear resposta: {text}\")\n",
    "                return []\n",
    "            \n",
    "            detections = []\n",
    "            for r in results:\n",
    "                bbox = tuple(int(x) for x in r['bbox'])\n",
    "                detections.append(Detection(\n",
    "                    bbox=bbox,\n",
    "                    mask=None,\n",
    "                    confidence=r.get('confidence', 0.8),\n",
    "                    label=r.get('description', 'fossil')\n",
    "                ))\n",
    "            \n",
    "            return detections\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro na API: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def detect_and_segment(self,\n",
    "                           img: np.ndarray,\n",
    "                           sam_detector: 'SAMDetector',\n",
    "                           context: str = \"Cambrian fossil sponges\") -> List[Detection]:\n",
    "        \"\"\"\n",
    "        Claude para detecção + SAM para segmentação.\n",
    "        Melhor dos dois mundos!\n",
    "        \"\"\"\n",
    "        # Claude detecta bboxes\n",
    "        detections = self.detect(img, context)\n",
    "        \n",
    "        if not detections:\n",
    "            return []\n",
    "        \n",
    "        # SAM segmenta cada bbox\n",
    "        refined_detections = []\n",
    "        \n",
    "        for det in detections:\n",
    "            sam_results = sam_detector.segment_with_box(img, det.bbox)\n",
    "            if sam_results:\n",
    "                sam_det = sam_results[0]\n",
    "                refined_detections.append(Detection(\n",
    "                    bbox=sam_det.bbox,\n",
    "                    mask=sam_det.mask,\n",
    "                    confidence=det.confidence,\n",
    "                    label=det.label\n",
    "                ))\n",
    "            else:\n",
    "                refined_detections.append(det)\n",
    "        \n",
    "        return refined_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Exemplo Claude Vision\n",
    "# ============================================\n",
    "\n",
    "# import os\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sua-api-key\"\n",
    "\n",
    "# claude_detector = ClaudeVisionDetector()\n",
    "# img = load_image(\"./sponge_images/GM1405_2.JPG\")\n",
    "\n",
    "# # Apenas detecção\n",
    "# detections = claude_detector.detect(img, context=\"Leptomitid sponges from Burgess Shale\")\n",
    "\n",
    "# # Ou detecção + segmentação com SAM\n",
    "# # detections = claude_detector.detect_and_segment(img, sam_detector)\n",
    "\n",
    "# visualize_detections(img, detections, \"Claude Vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. YOLOv8 - Fine-tuning com Poucas Amostras\n",
    "\n",
    "Se você tiver algumas anotações (~20-50 imagens), pode treinar YOLO rapidamente.\n",
    "\n",
    "### Workflow:\n",
    "1. Anotar imagens com LabelImg ou Roboflow\n",
    "2. Fine-tune YOLOv8 pré-treinado\n",
    "3. Inferência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODetector:\n",
    "    \"\"\"\n",
    "    YOLOv8 para detecção de fósseis.\n",
    "    Pode usar modelo pré-treinado ou fine-tuned.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = \"yolov8n.pt\"):\n",
    "        try:\n",
    "            from ultralytics import YOLO\n",
    "            self.model = YOLO(model_path)\n",
    "            self.available = True\n",
    "            print(f\"YOLOv8 carregado: {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"YOLOv8 não disponível: {e}\")\n",
    "            self.available = False\n",
    "    \n",
    "    def detect(self, \n",
    "               img: np.ndarray,\n",
    "               conf_threshold: float = 0.25) -> List[Detection]:\n",
    "        \"\"\"\n",
    "        Detecta objetos na imagem.\n",
    "        \n",
    "        Nota: Com modelo genérico, vai detectar classes do COCO.\n",
    "        Para fósseis, precisa fine-tuning.\n",
    "        \"\"\"\n",
    "        if not self.available:\n",
    "            return []\n",
    "        \n",
    "        results = self.model(img, conf=conf_threshold, verbose=False)\n",
    "        \n",
    "        detections = []\n",
    "        \n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            if boxes is None:\n",
    "                continue\n",
    "                \n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                conf = float(box.conf[0])\n",
    "                cls = int(box.cls[0])\n",
    "                label = self.model.names[cls]\n",
    "                \n",
    "                # Se tiver máscara (YOLO-seg)\n",
    "                mask = None\n",
    "                if hasattr(r, 'masks') and r.masks is not None:\n",
    "                    mask = r.masks.data[0].cpu().numpy()\n",
    "                \n",
    "                detections.append(Detection(\n",
    "                    bbox=(x1, y1, x2, y2),\n",
    "                    mask=mask,\n",
    "                    confidence=conf,\n",
    "                    label=label\n",
    "                ))\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_dataset_yaml(data_dir: str, \n",
    "                            classes: List[str] = [\"fossil_sponge\"],\n",
    "                            output_path: str = \"dataset.yaml\"):\n",
    "        \"\"\"\n",
    "        Cria arquivo YAML para treinar YOLO.\n",
    "        \n",
    "        Estrutura esperada:\n",
    "        data_dir/\n",
    "        ├── images/\n",
    "        │   ├── train/\n",
    "        │   └── val/\n",
    "        └── labels/\n",
    "            ├── train/\n",
    "            └── val/\n",
    "        \"\"\"\n",
    "        yaml_content = f\"\"\"\n",
    "path: {data_dir}\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "names:\n",
    "\"\"\"\n",
    "        for i, cls in enumerate(classes):\n",
    "            yaml_content += f\"  {i}: {cls}\\n\"\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(yaml_content)\n",
    "        \n",
    "        print(f\"Dataset YAML criado: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def train(self,\n",
    "              data_yaml: str,\n",
    "              epochs: int = 50,\n",
    "              imgsz: int = 640,\n",
    "              batch: int = 8,\n",
    "              output_dir: str = \"./yolo_training\"):\n",
    "        \"\"\"\n",
    "        Fine-tune YOLO no dataset de fósseis.\n",
    "        \"\"\"\n",
    "        if not self.available:\n",
    "            print(\"YOLO não disponível\")\n",
    "            return\n",
    "        \n",
    "        results = self.model.train(\n",
    "            data=data_yaml,\n",
    "            epochs=epochs,\n",
    "            imgsz=imgsz,\n",
    "            batch=batch,\n",
    "            project=output_dir,\n",
    "            name=\"fossil_detector\",\n",
    "            pretrained=True,\n",
    "            patience=10,  # Early stopping\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        print(f\"Treinamento completo! Modelo em: {output_dir}/fossil_detector/weights/best.pt\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Exemplo YOLOv8 Fine-tuning\n",
    "# ============================================\n",
    "\n",
    "# 1. Criar estrutura de dados (precisa anotar manualmente primeiro)\n",
    "# YOLODetector.create_dataset_yaml(\n",
    "#     data_dir=\"./fossil_dataset\",\n",
    "#     classes=[\"fossil_sponge\", \"matrix\"],\n",
    "#     output_path=\"fossil_dataset.yaml\"\n",
    "# )\n",
    "\n",
    "# 2. Treinar\n",
    "# yolo = YOLODetector(\"yolov8n.pt\")  # Modelo nano (rápido)\n",
    "# yolo.train(\"fossil_dataset.yaml\", epochs=50)\n",
    "\n",
    "# 3. Usar modelo treinado\n",
    "# yolo_trained = YOLODetector(\"./yolo_training/fossil_detector/weights/best.pt\")\n",
    "# detections = yolo_trained.detect(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Pipeline Unificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FossilDetectionPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline unificado para detecção de fósseis.\n",
    "    Escolhe automaticamente o melhor método disponível.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 sam_checkpoint: str = None,\n",
    "                 grounding_checkpoint: str = None,\n",
    "                 yolo_checkpoint: str = None,\n",
    "                 use_claude: bool = False,\n",
    "                 claude_api_key: str = None):\n",
    "        \n",
    "        self.detectors = {}\n",
    "        \n",
    "        # SAM\n",
    "        if sam_checkpoint and Path(sam_checkpoint).exists():\n",
    "            try:\n",
    "                self.detectors['sam'] = SAMDetector(sam_checkpoint)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar SAM: {e}\")\n",
    "        \n",
    "        # YOLO\n",
    "        if yolo_checkpoint:\n",
    "            try:\n",
    "                self.detectors['yolo'] = YOLODetector(yolo_checkpoint)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao carregar YOLO: {e}\")\n",
    "        \n",
    "        # Claude\n",
    "        if use_claude:\n",
    "            try:\n",
    "                self.detectors['claude'] = ClaudeVisionDetector(claude_api_key)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao inicializar Claude: {e}\")\n",
    "        \n",
    "        print(f\"Detectores disponíveis: {list(self.detectors.keys())}\")\n",
    "    \n",
    "    def detect(self,\n",
    "               img: np.ndarray,\n",
    "               method: str = 'auto',\n",
    "               **kwargs) -> List[Detection]:\n",
    "        \"\"\"\n",
    "        Detecta fósseis usando o método especificado.\n",
    "        \n",
    "        Args:\n",
    "            img: Imagem RGB\n",
    "            method: 'auto', 'sam', 'yolo', 'claude', 'claude+sam'\n",
    "        \"\"\"\n",
    "        if method == 'auto':\n",
    "            # Prioridade: YOLO treinado > Claude+SAM > SAM auto\n",
    "            if 'yolo' in self.detectors:\n",
    "                method = 'yolo'\n",
    "            elif 'claude' in self.detectors and 'sam' in self.detectors:\n",
    "                method = 'claude+sam'\n",
    "            elif 'sam' in self.detectors:\n",
    "                method = 'sam'\n",
    "            elif 'claude' in self.detectors:\n",
    "                method = 'claude'\n",
    "            else:\n",
    "                print(\"Nenhum detector disponível!\")\n",
    "                return []\n",
    "        \n",
    "        print(f\"Usando método: {method}\")\n",
    "        \n",
    "        if method == 'sam' and 'sam' in self.detectors:\n",
    "            return self.detectors['sam'].segment_automatic(img, **kwargs)\n",
    "        \n",
    "        elif method == 'yolo' and 'yolo' in self.detectors:\n",
    "            return self.detectors['yolo'].detect(img, **kwargs)\n",
    "        \n",
    "        elif method == 'claude' and 'claude' in self.detectors:\n",
    "            return self.detectors['claude'].detect(img, **kwargs)\n",
    "        \n",
    "        elif method == 'claude+sam':\n",
    "            if 'claude' in self.detectors and 'sam' in self.detectors:\n",
    "                return self.detectors['claude'].detect_and_segment(\n",
    "                    img, self.detectors['sam'], **kwargs\n",
    "                )\n",
    "        \n",
    "        print(f\"Método {method} não disponível\")\n",
    "        return []\n",
    "    \n",
    "    def process_and_extract_windows(self,\n",
    "                                    img: np.ndarray,\n",
    "                                    method: str = 'auto',\n",
    "                                    window_sizes: List[int] = [64, 128, 256],\n",
    "                                    stride: int = 32,\n",
    "                                    min_content_ratio: float = 0.5) -> Dict:\n",
    "        \"\"\"\n",
    "        Detecta fósseis e extrai janelas de convolução.\n",
    "        \"\"\"\n",
    "        detections = self.detect(img, method=method)\n",
    "        \n",
    "        all_windows = []\n",
    "        \n",
    "        for i, det in enumerate(detections):\n",
    "            if det.mask is None:\n",
    "                continue\n",
    "            \n",
    "            # Extrair ROI\n",
    "            x1, y1, x2, y2 = det.bbox\n",
    "            roi_img = img[y1:y2, x1:x2]\n",
    "            roi_mask = det.mask[y1:y2, x1:x2]\n",
    "            \n",
    "            h, w = roi_img.shape[:2]\n",
    "            \n",
    "            # Extrair janelas\n",
    "            for window_size in window_sizes:\n",
    "                if window_size > min(h, w):\n",
    "                    continue\n",
    "                \n",
    "                for y in range(0, h - window_size + 1, stride):\n",
    "                    for x in range(0, w - window_size + 1, stride):\n",
    "                        window_img = roi_img[y:y+window_size, x:x+window_size]\n",
    "                        window_mask = roi_mask[y:y+window_size, x:x+window_size]\n",
    "                        \n",
    "                        content_ratio = np.sum(window_mask > 0) / (window_size ** 2)\n",
    "                        \n",
    "                        if content_ratio >= min_content_ratio:\n",
    "                            all_windows.append({\n",
    "                                'image': window_img,\n",
    "                                'position': (x + x1, y + y1),\n",
    "                                'window_size': window_size,\n",
    "                                'fossil_id': i,\n",
    "                                'content_ratio': content_ratio\n",
    "                            })\n",
    "        \n",
    "        return {\n",
    "            'detections': detections,\n",
    "            'windows': all_windows\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Helper: Anotação Semi-Automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation_from_detection(img_path: str,\n",
    "                                     detections: List[Detection],\n",
    "                                     output_dir: str,\n",
    "                                     format: str = 'yolo'):\n",
    "    \"\"\"\n",
    "    Converte detecções para formato de anotação YOLO.\n",
    "    Útil para criar dataset de treino a partir de detecções automáticas.\n",
    "    \n",
    "    Args:\n",
    "        img_path: Caminho da imagem\n",
    "        detections: Lista de detecções\n",
    "        output_dir: Diretório de saída\n",
    "        format: 'yolo' ou 'coco'\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    img = load_image(img_path)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    images_dir = output_path / 'images' / 'train'\n",
    "    labels_dir = output_path / 'labels' / 'train'\n",
    "    \n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copiar imagem\n",
    "    img_name = Path(img_path).name\n",
    "    shutil.copy(img_path, images_dir / img_name)\n",
    "    \n",
    "    # Criar arquivo de labels\n",
    "    label_file = labels_dir / (Path(img_path).stem + '.txt')\n",
    "    \n",
    "    with open(label_file, 'w') as f:\n",
    "        for det in detections:\n",
    "            x1, y1, x2, y2 = det.bbox\n",
    "            \n",
    "            # Converter para formato YOLO (normalizado)\n",
    "            cx = ((x1 + x2) / 2) / w\n",
    "            cy = ((y1 + y2) / 2) / h\n",
    "            bw = (x2 - x1) / w\n",
    "            bh = (y2 - y1) / h\n",
    "            \n",
    "            # class_id x_center y_center width height\n",
    "            f.write(f\"0 {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\\n\")\n",
    "    \n",
    "    print(f\"Anotação criada: {label_file}\")\n",
    "\n",
    "\n",
    "def batch_create_annotations(image_dir: str,\n",
    "                             pipeline: FossilDetectionPipeline,\n",
    "                             output_dir: str,\n",
    "                             method: str = 'auto'):\n",
    "    \"\"\"\n",
    "    Cria anotações para todas as imagens de um diretório.\n",
    "    Depois você pode revisar manualmente e corrigir.\n",
    "    \"\"\"\n",
    "    image_paths = list(Path(image_dir).glob('*.jpg')) + \\\n",
    "                  list(Path(image_dir).glob('*.JPG')) + \\\n",
    "                  list(Path(image_dir).glob('*.png'))\n",
    "    \n",
    "    print(f\"Processando {len(image_paths)} imagens...\")\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            img = load_image(str(img_path))\n",
    "            detections = pipeline.detect(img, method=method)\n",
    "            \n",
    "            if detections:\n",
    "                create_annotation_from_detection(\n",
    "                    str(img_path), detections, output_dir\n",
    "                )\n",
    "                print(f\"  {img_path.name}: {len(detections)} detecções\")\n",
    "            else:\n",
    "                print(f\"  {img_path.name}: nenhuma detecção\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Erro em {img_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nAnotações salvas em {output_dir}\")\n",
    "    print(\"Revise manualmente antes de treinar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Exemplo Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# WORKFLOW RECOMENDADO\n",
    "# ============================================\n",
    "\n",
    "# OPÇÃO A: Sem treinamento (mais simples)\n",
    "# -----------------------------------------\n",
    "# 1. Usar Claude para detectar bboxes\n",
    "# 2. SAM para segmentar dentro dos bboxes\n",
    "\n",
    "# pipeline = FossilDetectionPipeline(\n",
    "#     sam_checkpoint=\"./models/sam_vit_b_01ec64.pth\",\n",
    "#     use_claude=True\n",
    "# )\n",
    "\n",
    "# img = load_image(\"./sponge_images/GM1405_2.JPG\")\n",
    "# result = pipeline.process_and_extract_windows(img, method='claude+sam')\n",
    "\n",
    "# print(f\"Detectados: {len(result['detections'])} fósseis\")\n",
    "# print(f\"Extraídas: {len(result['windows'])} janelas\")\n",
    "\n",
    "# visualize_detections(img, result['detections'], \"Pipeline Completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPÇÃO B: Com fine-tuning YOLO (mais robusto)\n",
    "# -----------------------------------------\n",
    "# 1. Usar Claude+SAM para gerar anotações iniciais\n",
    "# 2. Revisar manualmente\n",
    "# 3. Treinar YOLO\n",
    "# 4. Usar YOLO treinado\n",
    "\n",
    "# # Passo 1: Gerar anotações\n",
    "# pipeline = FossilDetectionPipeline(sam_checkpoint=\"./models/sam_vit_b_01ec64.pth\", use_claude=True)\n",
    "# batch_create_annotations(\"./raw_images\", pipeline, \"./fossil_dataset\", method='claude+sam')\n",
    "\n",
    "# # Passo 2: REVISAR MANUALMENTE AS ANOTAÇÕES!\n",
    "# # Use LabelImg ou Roboflow\n",
    "\n",
    "# # Passo 3: Treinar\n",
    "# YOLODetector.create_dataset_yaml(\"./fossil_dataset\", [\"fossil_sponge\"])\n",
    "# yolo = YOLODetector(\"yolov8n.pt\")\n",
    "# yolo.train(\"dataset.yaml\", epochs=50)\n",
    "\n",
    "# # Passo 4: Usar\n",
    "# trained_yolo = YOLODetector(\"./yolo_training/fossil_detector/weights/best.pt\")\n",
    "# detections = trained_yolo.detect(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparação dos Métodos\n",
    "\n",
    "| Método | Requer GPU? | Requer Treino? | Precisão | Setup |\n",
    "|--------|-------------|----------------|----------|-------|\n",
    "| SAM (auto) | Recomendado | Não | Média* | Médio |\n",
    "| SAM (prompt) | Recomendado | Não | Alta | Fácil |\n",
    "| Grounding DINO + SAM | Sim | Não | Alta | Complexo |\n",
    "| Claude Vision | Não | Não | Boa | Muito fácil |\n",
    "| Claude + SAM | Recomendado | Não | Alta | Fácil |\n",
    "| YOLOv8 fine-tuned | Sim | Sim (~50 imgs) | Muito alta | Médio |\n",
    "\n",
    "*SAM automático precisa de filtros para selecionar fósseis vs background\n",
    "\n",
    "### Recomendação:\n",
    "\n",
    "1. **Começar com**: Claude + SAM (sem setup, boa precisão)\n",
    "2. **Para produção**: Treinar YOLOv8 com anotações geradas semi-automaticamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Downloads dos Modelos\n",
    "\n",
    "```bash\n",
    "# Criar diretório\n",
    "mkdir -p models && cd models\n",
    "\n",
    "# SAM ViT-B (375MB) - recomendado para começar\n",
    "wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "\n",
    "# SAM ViT-H (2.4GB) - mais preciso\n",
    "wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "\n",
    "# Grounding DINO (694MB)\n",
    "wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
