{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção Automática de Objetos em Lâminas\n",
    "\n",
    "Pipeline 100% automático:\n",
    "1. SAM detecta TODOS os objetos\n",
    "2. Filtros morfológicos removem ruído/background\n",
    "3. Extrai e salva cada objeto separadamente\n",
    "4. Pronto para clusterização no SAMI\n",
    "\n",
    "**Zero prompts. Zero intervenção.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependências\n",
    "# !pip install segment-anything opencv-python numpy matplotlib torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download do modelo SAM (executar uma vez)\n",
    "# !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -O sam_vit_b.pth\n",
    "# !echo \"Download completo: sam_vit_b.pth (375MB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuração (ajuste uma vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuração do pipeline - ajuste conforme suas imagens\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    sam_checkpoint: str = \"sam_vit_b.pth\"\n",
    "    input_dir: str = \"./input_images\"      # Onde estão as lâminas\n",
    "    output_dir: str = \"./detected_objects\"  # Onde salvar objetos extraídos\n",
    "    \n",
    "    # Filtros de área (em pixels)\n",
    "    min_area: int = 5000        # Objetos menores são descartados\n",
    "    max_area_ratio: float = 0.6  # Máximo 60% da imagem (remove background)\n",
    "    \n",
    "    # Filtros morfológicos\n",
    "    min_solidity: float = 0.3    # Quão \"sólido\" é o objeto (0-1)\n",
    "    min_aspect_ratio: float = 1.2 # Fósseis são geralmente alongados\n",
    "    max_aspect_ratio: float = 15.0\n",
    "    \n",
    "    # SAM parameters\n",
    "    points_per_side: int = 32    # Densidade de pontos (mais = mais detecções)\n",
    "    pred_iou_thresh: float = 0.86\n",
    "    stability_score_thresh: float = 0.92\n",
    "    \n",
    "    # Output\n",
    "    padding: int = 20            # Margem ao redor do objeto extraído\n",
    "    save_masks: bool = True      # Salvar máscaras junto com ROIs\n",
    "    save_visualization: bool = True\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core: Detector Automático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DetectedObject:\n",
    "    \"\"\"Objeto detectado na lâmina\"\"\"\n",
    "    id: int\n",
    "    bbox: Tuple[int, int, int, int]  # x1, y1, x2, y2\n",
    "    mask: np.ndarray\n",
    "    area: int\n",
    "    centroid: Tuple[int, int]\n",
    "    aspect_ratio: float\n",
    "    solidity: float\n",
    "    iou_score: float\n",
    "    stability_score: float\n",
    "    roi_image: Optional[np.ndarray] = None\n",
    "    roi_mask: Optional[np.ndarray] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomaticObjectDetector:\n",
    "    \"\"\"\n",
    "    Detector 100% automático usando SAM.\n",
    "    Detecta todos os objetos e filtra por propriedades morfológicas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self._load_sam()\n",
    "    \n",
    "    def _load_sam(self):\n",
    "        \"\"\"Carrega modelo SAM\"\"\"\n",
    "        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "        \n",
    "        print(\"Carregando SAM...\")\n",
    "        \n",
    "        # Detectar tipo do modelo pelo nome do arquivo\n",
    "        checkpoint = self.config.sam_checkpoint\n",
    "        if 'vit_h' in checkpoint:\n",
    "            model_type = 'vit_h'\n",
    "        elif 'vit_l' in checkpoint:\n",
    "            model_type = 'vit_l'\n",
    "        else:\n",
    "            model_type = 'vit_b'\n",
    "        \n",
    "        sam = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "        sam.to(DEVICE)\n",
    "        \n",
    "        self.mask_generator = SamAutomaticMaskGenerator(\n",
    "            sam,\n",
    "            points_per_side=self.config.points_per_side,\n",
    "            pred_iou_thresh=self.config.pred_iou_thresh,\n",
    "            stability_score_thresh=self.config.stability_score_thresh,\n",
    "            min_mask_region_area=self.config.min_area,\n",
    "        )\n",
    "        \n",
    "        print(f\"SAM carregado ({model_type}) em {DEVICE}\")\n",
    "    \n",
    "    def _compute_morphological_features(self, mask: np.ndarray) -> dict:\n",
    "        \"\"\"Calcula features morfológicas de uma máscara\"\"\"\n",
    "        # Encontrar contornos\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask.astype(np.uint8), \n",
    "            cv2.RETR_EXTERNAL, \n",
    "            cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        \n",
    "        if not contours:\n",
    "            return None\n",
    "        \n",
    "        # Pegar maior contorno\n",
    "        contour = max(contours, key=cv2.contourArea)\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        if area < 100:\n",
    "            return None\n",
    "        \n",
    "        # Bounding box\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Aspect ratio\n",
    "        aspect_ratio = max(w, h) / (min(w, h) + 1e-6)\n",
    "        \n",
    "        # Solidity (área / área do convex hull)\n",
    "        hull = cv2.convexHull(contour)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        solidity = area / (hull_area + 1e-6)\n",
    "        \n",
    "        # Centroid\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] > 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "        else:\n",
    "            cx, cy = x + w // 2, y + h // 2\n",
    "        \n",
    "        return {\n",
    "            'bbox': (x, y, x + w, y + h),\n",
    "            'area': area,\n",
    "            'aspect_ratio': aspect_ratio,\n",
    "            'solidity': solidity,\n",
    "            'centroid': (cx, cy),\n",
    "            'contour': contour\n",
    "        }\n",
    "    \n",
    "    def _filter_detection(self, features: dict, img_area: int) -> bool:\n",
    "        \"\"\"Verifica se detecção passa nos filtros\"\"\"\n",
    "        if features is None:\n",
    "            return False\n",
    "        \n",
    "        # Filtro de área\n",
    "        if features['area'] < self.config.min_area:\n",
    "            return False\n",
    "        \n",
    "        if features['area'] > img_area * self.config.max_area_ratio:\n",
    "            return False\n",
    "        \n",
    "        # Filtro de aspect ratio\n",
    "        if features['aspect_ratio'] < self.config.min_aspect_ratio:\n",
    "            return False\n",
    "        \n",
    "        if features['aspect_ratio'] > self.config.max_aspect_ratio:\n",
    "            return False\n",
    "        \n",
    "        # Filtro de solidity\n",
    "        if features['solidity'] < self.config.min_solidity:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _extract_roi(self, img: np.ndarray, mask: np.ndarray, \n",
    "                     bbox: Tuple[int, int, int, int]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Extrai ROI com padding\"\"\"\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        h, w = img.shape[:2]\n",
    "        pad = self.config.padding\n",
    "        \n",
    "        # Aplicar padding\n",
    "        x1 = max(0, x1 - pad)\n",
    "        y1 = max(0, y1 - pad)\n",
    "        x2 = min(w, x2 + pad)\n",
    "        y2 = min(h, y2 + pad)\n",
    "        \n",
    "        roi_img = img[y1:y2, x1:x2].copy()\n",
    "        roi_mask = mask[y1:y2, x1:x2].copy()\n",
    "        \n",
    "        return roi_img, roi_mask\n",
    "    \n",
    "    def detect(self, img: np.ndarray) -> List[DetectedObject]:\n",
    "        \"\"\"\n",
    "        Detecta todos os objetos na imagem.\n",
    "        \n",
    "        Args:\n",
    "            img: Imagem RGB (numpy array)\n",
    "            \n",
    "        Returns:\n",
    "            Lista de objetos detectados\n",
    "        \"\"\"\n",
    "        h, w = img.shape[:2]\n",
    "        img_area = h * w\n",
    "        \n",
    "        # SAM gera todas as máscaras\n",
    "        masks_data = self.mask_generator.generate(img)\n",
    "        \n",
    "        objects = []\n",
    "        obj_id = 0\n",
    "        \n",
    "        for mask_info in masks_data:\n",
    "            mask = mask_info['segmentation']\n",
    "            \n",
    "            # Calcular features morfológicas\n",
    "            features = self._compute_morphological_features(mask)\n",
    "            \n",
    "            # Aplicar filtros\n",
    "            if not self._filter_detection(features, img_area):\n",
    "                continue\n",
    "            \n",
    "            # Extrair ROI\n",
    "            roi_img, roi_mask = self._extract_roi(img, mask, features['bbox'])\n",
    "            \n",
    "            obj = DetectedObject(\n",
    "                id=obj_id,\n",
    "                bbox=features['bbox'],\n",
    "                mask=mask,\n",
    "                area=features['area'],\n",
    "                centroid=features['centroid'],\n",
    "                aspect_ratio=features['aspect_ratio'],\n",
    "                solidity=features['solidity'],\n",
    "                iou_score=mask_info['predicted_iou'],\n",
    "                stability_score=mask_info['stability_score'],\n",
    "                roi_image=roi_img,\n",
    "                roi_mask=roi_mask\n",
    "            )\n",
    "            \n",
    "            objects.append(obj)\n",
    "            obj_id += 1\n",
    "        \n",
    "        # Ordenar por área (maior primeiro)\n",
    "        objects.sort(key=lambda x: x.area, reverse=True)\n",
    "        \n",
    "        # Reatribuir IDs após ordenação\n",
    "        for i, obj in enumerate(objects):\n",
    "            obj.id = i\n",
    "        \n",
    "        return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline de Processamento em Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"\n",
    "    Processa múltiplas imagens automaticamente.\n",
    "    Salva todos os objetos detectados organizados por imagem.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.detector = AutomaticObjectDetector(config)\n",
    "        \n",
    "        # Criar diretórios de saída\n",
    "        self.output_path = Path(config.output_dir)\n",
    "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        (self.output_path / 'rois').mkdir(exist_ok=True)\n",
    "        if config.save_masks:\n",
    "            (self.output_path / 'masks').mkdir(exist_ok=True)\n",
    "        if config.save_visualization:\n",
    "            (self.output_path / 'visualizations').mkdir(exist_ok=True)\n",
    "    \n",
    "    def _load_image(self, path: str) -> np.ndarray:\n",
    "        \"\"\"Carrega imagem em RGB\"\"\"\n",
    "        img = cv2.imread(str(path))\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Não foi possível carregar: {path}\")\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    def _save_object(self, obj: DetectedObject, image_name: str):\n",
    "        \"\"\"Salva um objeto detectado\"\"\"\n",
    "        prefix = f\"{image_name}_obj{obj.id:03d}\"\n",
    "        \n",
    "        # Salvar ROI\n",
    "        roi_path = self.output_path / 'rois' / f\"{prefix}.png\"\n",
    "        cv2.imwrite(str(roi_path), cv2.cvtColor(obj.roi_image, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # Salvar máscara\n",
    "        if self.config.save_masks:\n",
    "            mask_path = self.output_path / 'masks' / f\"{prefix}_mask.png\"\n",
    "            cv2.imwrite(str(mask_path), (obj.roi_mask * 255).astype(np.uint8))\n",
    "    \n",
    "    def _save_visualization(self, img: np.ndarray, objects: List[DetectedObject], \n",
    "                            image_name: str):\n",
    "        \"\"\"Salva visualização com todas as detecções\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        # Imagem com bboxes\n",
    "        vis = img.copy()\n",
    "        for obj in objects:\n",
    "            x1, y1, x2, y2 = obj.bbox\n",
    "            cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "            cv2.putText(vis, f\"#{obj.id}\", (x1, y1-10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "        \n",
    "        axes[0].imshow(vis)\n",
    "        axes[0].set_title(f\"{image_name}: {len(objects)} objetos detectados\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Máscaras combinadas\n",
    "        overlay = img.copy().astype(float)\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, max(len(objects), 1)))\n",
    "        \n",
    "        for obj, color in zip(objects, colors):\n",
    "            mask_3d = np.stack([obj.mask] * 3, axis=-1)\n",
    "            overlay = np.where(mask_3d, \n",
    "                              overlay * 0.5 + np.array(color[:3]) * 255 * 0.5, \n",
    "                              overlay)\n",
    "        \n",
    "        axes[1].imshow(overlay.astype(np.uint8))\n",
    "        axes[1].set_title(\"Máscaras de segmentação\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_path / 'visualizations' / f\"{image_name}_detection.png\",\n",
    "                   dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _save_metadata(self, all_results: dict):\n",
    "        \"\"\"Salva metadados de todas as detecções\"\"\"\n",
    "        metadata = {\n",
    "            'config': {\n",
    "                'min_area': self.config.min_area,\n",
    "                'max_area_ratio': self.config.max_area_ratio,\n",
    "                'min_solidity': self.config.min_solidity,\n",
    "                'min_aspect_ratio': self.config.min_aspect_ratio,\n",
    "                'max_aspect_ratio': self.config.max_aspect_ratio,\n",
    "            },\n",
    "            'images': {}\n",
    "        }\n",
    "        \n",
    "        for image_name, objects in all_results.items():\n",
    "            metadata['images'][image_name] = {\n",
    "                'n_objects': len(objects),\n",
    "                'objects': [\n",
    "                    {\n",
    "                        'id': obj.id,\n",
    "                        'bbox': obj.bbox,\n",
    "                        'area': obj.area,\n",
    "                        'aspect_ratio': round(obj.aspect_ratio, 2),\n",
    "                        'solidity': round(obj.solidity, 3),\n",
    "                        'iou_score': round(obj.iou_score, 3),\n",
    "                        'roi_file': f\"{image_name}_obj{obj.id:03d}.png\"\n",
    "                    }\n",
    "                    for obj in objects\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "        with open(self.output_path / 'metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    def process(self, image_paths: List[str] = None) -> dict:\n",
    "        \"\"\"\n",
    "        Processa todas as imagens.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: Lista de caminhos (ou None para usar input_dir)\n",
    "            \n",
    "        Returns:\n",
    "            Dict com resultados por imagem\n",
    "        \"\"\"\n",
    "        # Listar imagens\n",
    "        if image_paths is None:\n",
    "            input_path = Path(self.config.input_dir)\n",
    "            image_paths = list(input_path.glob('*.jpg')) + \\\n",
    "                         list(input_path.glob('*.JPG')) + \\\n",
    "                         list(input_path.glob('*.jpeg')) + \\\n",
    "                         list(input_path.glob('*.png'))\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSAMENTO AUTOMÁTICO\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Imagens encontradas: {len(image_paths)}\")\n",
    "        print(f\"Output: {self.output_path}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        all_results = {}\n",
    "        total_objects = 0\n",
    "        \n",
    "        for img_path in tqdm(image_paths, desc=\"Processando\"):\n",
    "            image_name = Path(img_path).stem\n",
    "            \n",
    "            try:\n",
    "                # Carregar imagem\n",
    "                img = self._load_image(img_path)\n",
    "                \n",
    "                # Detectar objetos\n",
    "                objects = self.detector.detect(img)\n",
    "                \n",
    "                # Salvar cada objeto\n",
    "                for obj in objects:\n",
    "                    self._save_object(obj, image_name)\n",
    "                \n",
    "                # Salvar visualização\n",
    "                if self.config.save_visualization:\n",
    "                    self._save_visualization(img, objects, image_name)\n",
    "                \n",
    "                all_results[image_name] = objects\n",
    "                total_objects += len(objects)\n",
    "                \n",
    "                print(f\"  {image_name}: {len(objects)} objetos\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ERRO em {image_name}: {e}\")\n",
    "                all_results[image_name] = []\n",
    "        \n",
    "        # Salvar metadados\n",
    "        self._save_metadata(all_results)\n",
    "        \n",
    "        # Resumo\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESUMO\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Imagens processadas: {len(all_results)}\")\n",
    "        print(f\"Total de objetos: {total_objects}\")\n",
    "        print(f\"Média por imagem: {total_objects/max(len(all_results),1):.1f}\")\n",
    "        print(f\"\\nArquivos salvos em: {self.output_path}\")\n",
    "        print(f\"  - rois/: {total_objects} imagens de objetos\")\n",
    "        if self.config.save_masks:\n",
    "            print(f\"  - masks/: {total_objects} máscaras\")\n",
    "        if self.config.save_visualization:\n",
    "            print(f\"  - visualizations/: {len(all_results)} visualizações\")\n",
    "        print(f\"  - metadata.json: metadados completos\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparação para SAMI (Clusterização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_sami(detected_objects_dir: str, \n",
    "                     sami_dataset_dir: str,\n",
    "                     class_name: str = \"unknown\"):\n",
    "    \"\"\"\n",
    "    Organiza objetos detectados no formato ImageFolder para SAMI.\n",
    "    \n",
    "    Estrutura de saída:\n",
    "    sami_dataset_dir/\n",
    "    └── unknown/\n",
    "        ├── image1_obj000.png\n",
    "        ├── image1_obj001.png\n",
    "        └── ...\n",
    "    \n",
    "    Depois de rodar clusterização no SAMI, você pode reorganizar\n",
    "    por cluster para treinar classificadores.\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    src = Path(detected_objects_dir) / 'rois'\n",
    "    dst = Path(sami_dataset_dir) / class_name\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copiar todas as ROIs\n",
    "    count = 0\n",
    "    for img_file in src.glob('*.png'):\n",
    "        shutil.copy(img_file, dst / img_file.name)\n",
    "        count += 1\n",
    "    \n",
    "    print(f\"Copiados {count} objetos para {dst}\")\n",
    "    print(f\"\\nPronto para rodar SAMI:\")\n",
    "    print(f\"  from SAMI import run_full_evaluation\")\n",
    "    print(f\"  results = run_full_evaluation('{sami_dataset_dir}')\")\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. EXECUTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURAÇÃO - AJUSTE AQUI\n",
    "# ============================================\n",
    "\n",
    "config = Config(\n",
    "    # Paths\n",
    "    sam_checkpoint=\"sam_vit_b.pth\",     # Caminho para o modelo SAM\n",
    "    input_dir=\"./input_images\",          # Pasta com suas lâminas\n",
    "    output_dir=\"./detected_objects\",     # Onde salvar resultados\n",
    "    \n",
    "    # Filtros - ajuste conforme suas imagens\n",
    "    min_area=5000,           # Área mínima em pixels\n",
    "    max_area_ratio=0.6,      # Máximo 60% da imagem\n",
    "    min_solidity=0.3,        # Objetos \"sólidos\"\n",
    "    min_aspect_ratio=1.2,    # Objetos alongados\n",
    "    max_aspect_ratio=15.0,\n",
    "    \n",
    "    # SAM\n",
    "    points_per_side=32,      # Mais pontos = mais detecções (mais lento)\n",
    "    \n",
    "    # Output\n",
    "    padding=20,\n",
    "    save_masks=True,\n",
    "    save_visualization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PROCESSAR TODAS AS IMAGENS\n",
    "# ============================================\n",
    "\n",
    "processor = BatchProcessor(config)\n",
    "results = processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREPARAR PARA SAMI\n",
    "# ============================================\n",
    "\n",
    "prepare_for_sami(\n",
    "    detected_objects_dir=\"./detected_objects\",\n",
    "    sami_dataset_dir=\"./sami_dataset\",\n",
    "    class_name=\"detected_objects\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualizar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detected_objects(output_dir: str, max_objects: int = 20):\n",
    "    \"\"\"Mostra grid com objetos detectados\"\"\"\n",
    "    rois_dir = Path(output_dir) / 'rois'\n",
    "    roi_files = list(rois_dir.glob('*.png'))[:max_objects]\n",
    "    \n",
    "    if not roi_files:\n",
    "        print(\"Nenhum objeto encontrado\")\n",
    "        return\n",
    "    \n",
    "    n_cols = 5\n",
    "    n_rows = (len(roi_files) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 and n_cols == 1 else axes\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < len(roi_files):\n",
    "            img = cv2.imread(str(roi_files[i]))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(roi_files[i].stem, fontsize=8)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Objetos Detectados ({len(roi_files)} de {len(list(rois_dir.glob('*.png')))})\", \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# show_detected_objects(\"./detected_objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_detections(output_dir: str):\n",
    "    \"\"\"Análise estatística das detecções\"\"\"\n",
    "    with open(Path(output_dir) / 'metadata.json') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    all_objects = []\n",
    "    for image_name, data in metadata['images'].items():\n",
    "        for obj in data['objects']:\n",
    "            obj['image'] = image_name\n",
    "            all_objects.append(obj)\n",
    "    \n",
    "    if not all_objects:\n",
    "        print(\"Nenhum objeto detectado\")\n",
    "        return\n",
    "    \n",
    "    # Estatísticas\n",
    "    areas = [o['area'] for o in all_objects]\n",
    "    aspects = [o['aspect_ratio'] for o in all_objects]\n",
    "    solidities = [o['solidity'] for o in all_objects]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].hist(areas, bins=30, edgecolor='black')\n",
    "    axes[0].set_xlabel('Área (pixels)')\n",
    "    axes[0].set_ylabel('Frequência')\n",
    "    axes[0].set_title('Distribuição de Área')\n",
    "    \n",
    "    axes[1].hist(aspects, bins=30, edgecolor='black')\n",
    "    axes[1].set_xlabel('Aspect Ratio')\n",
    "    axes[1].set_title('Distribuição de Aspect Ratio')\n",
    "    \n",
    "    axes[2].hist(solidities, bins=30, edgecolor='black')\n",
    "    axes[2].set_xlabel('Solidity')\n",
    "    axes[2].set_title('Distribuição de Solidity')\n",
    "    \n",
    "    plt.suptitle(f\"Estatísticas de {len(all_objects)} objetos detectados\", \n",
    "                fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nEstatísticas:\")\n",
    "    print(f\"  Área: {np.mean(areas):.0f} ± {np.std(areas):.0f} pixels\")\n",
    "    print(f\"  Aspect Ratio: {np.mean(aspects):.2f} ± {np.std(aspects):.2f}\")\n",
    "    print(f\"  Solidity: {np.mean(solidities):.3f} ± {np.std(solidities):.3f}\")\n",
    "\n",
    "# analyze_detections(\"./detected_objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notas\n",
    "\n",
    "### Workflow Completo:\n",
    "\n",
    "```\n",
    "1. Colocar lâminas em ./input_images/\n",
    "2. Executar este notebook (células 6)\n",
    "3. Verificar ./detected_objects/visualizations/\n",
    "4. Se necessário, ajustar filtros e re-executar\n",
    "5. Rodar SAMI para clusterização\n",
    "```\n",
    "\n",
    "### Parâmetros Importantes:\n",
    "\n",
    "| Parâmetro | Efeito | Ajuste se... |\n",
    "|-----------|--------|-------------|\n",
    "| `min_area` | Filtra objetos pequenos | Detectando muito ruído |\n",
    "| `max_area_ratio` | Filtra objetos grandes | Detectando background |\n",
    "| `min_aspect_ratio` | Exige objetos alongados | Fósseis são alongados |\n",
    "| `min_solidity` | Exige formas compactas | Filtra artefatos fragmentados |\n",
    "| `points_per_side` | Densidade de detecção | Mais = mais detecções (mais lento) |\n",
    "\n",
    "### Se estiver detectando muito background:\n",
    "- Aumentar `min_area`\n",
    "- Diminuir `max_area_ratio`\n",
    "- Aumentar `min_solidity`\n",
    "\n",
    "### Se estiver perdendo fósseis:\n",
    "- Diminuir `min_area`\n",
    "- Diminuir `min_aspect_ratio`\n",
    "- Aumentar `points_per_side`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
