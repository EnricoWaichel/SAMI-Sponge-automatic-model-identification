{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMI - Sponge Automatic Model Identification\n",
    "\n",
    "**Pipeline completo para identificação automática de espécimes de esponjas fósseis usando Vision Transformers**\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "1. [Setup e Dependências](#1-setup)\n",
    "2. [Vision Transformer](#2-vit)\n",
    "3. [Utilitários Gerais](#3-utils)\n",
    "4. [CBIR - Content-Based Image Retrieval](#4-cbir)\n",
    "5. [Utilitários de Avaliação](#5-eval)\n",
    "6. [Clustering Analysis](#6-clustering)\n",
    "7. [Multi-Scale Patch Clustering](#7-patches)\n",
    "8. [Pipeline de Avaliação](#8-pipeline)\n",
    "9. [Exemplo de Uso](#9-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup e Dependências <a id=\"1-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependências (se necessário)\n",
    "# !pip install torch torchvision numpy pandas matplotlib seaborn scikit-learn umap-learn opencv-python tqdm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports globais\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    f1_score, precision_score, recall_score, silhouette_score,\n",
    "    calinski_harabasz_score\n",
    ")\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"UMAP não disponível. Instale com: pip install umap-learn\")\n",
    "    UMAP_AVAILABLE = False\n",
    "\n",
    "# Configurações\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Vision Transformer <a id=\"2-vit\"></a>\n",
    "\n",
    "Implementação do Vision Transformer baseada em DINO/timm, adaptada do benchmark SCAMPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
    "                             attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                      act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer para SAMI\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768,\n",
    "                 depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, drop_rate=0.,\n",
    "                 attn_drop_rate=0., norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        torch.nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(N ** 0.5), int(N ** 0.5), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / N ** 0.5, h0 / N ** 0.5),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]  # CLS token\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk.attn(blk.norm1(x))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory functions para diferentes tamanhos de ViT\n",
    "\n",
    "def vit_tiny(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "def vit_small(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "def vit_base(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "def vit_large(patch_size=16, **kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=patch_size, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,\n",
    "        qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "print(\"Vision Transformer definido com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Utilitários Gerais <a id=\"3-utils\"></a>\n",
    "\n",
    "Funções para carregamento de imagens, pré-processamento e manipulação de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path: str) -> Image.Image:\n",
    "    \"\"\"Carrega imagem do disco\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_transform(img_size: int = 224, is_training: bool = False):\n",
    "    \"\"\"\n",
    "    Pipeline de transformação de imagens\n",
    "    \n",
    "    Args:\n",
    "        img_size: Tamanho alvo\n",
    "        is_training: Se True, aplica augmentations\n",
    "    \"\"\"\n",
    "    if is_training:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(int(img_size * 1.14)),\n",
    "            transforms.CenterCrop(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "def preprocess_image(img: Image.Image, img_size: int = 224) -> torch.Tensor:\n",
    "    \"\"\"Pré-processa uma imagem para input do modelo\"\"\"\n",
    "    transform = get_transform(img_size, is_training=False)\n",
    "    return transform(img)\n",
    "\n",
    "\n",
    "def load_dataset_images(data_path: str, \n",
    "                       img_size: int = 224) -> Tuple[torch.Tensor, List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    Carrega todas as imagens de um diretório organizado por classes\n",
    "    \n",
    "    Args:\n",
    "        data_path: Caminho raiz do dataset\n",
    "        img_size: Tamanho alvo das imagens\n",
    "    \n",
    "    Returns:\n",
    "        (images_tensor, image_paths, labels)\n",
    "    \"\"\"\n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    class_folders = sorted([d for d in data_path.iterdir() if d.is_dir()])\n",
    "    class_to_idx = {cls_folder.name: idx for idx, cls_folder in enumerate(class_folders)}\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    \n",
    "    transform = get_transform(img_size, is_training=False)\n",
    "    \n",
    "    print(f\"Carregando imagens de {len(class_folders)} classes...\")\n",
    "    \n",
    "    for class_folder in class_folders:\n",
    "        class_name = class_folder.name\n",
    "        class_idx = class_to_idx[class_name]\n",
    "        \n",
    "        image_files = list(class_folder.glob('*.jpg')) + \\\n",
    "                     list(class_folder.glob('*.jpeg')) + \\\n",
    "                     list(class_folder.glob('*.png'))\n",
    "        \n",
    "        print(f\"  {class_name}: {len(image_files)} imagens\")\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            img = load_image(str(img_file))\n",
    "            if img is not None:\n",
    "                img_tensor = transform(img)\n",
    "                images.append(img_tensor)\n",
    "                labels.append(class_idx)\n",
    "                image_paths.append(str(img_file))\n",
    "    \n",
    "    images_tensor = torch.stack(images)\n",
    "    \n",
    "    print(f\"\\nTotal de imagens: {len(images)}\")\n",
    "    print(f\"Shape do tensor: {images_tensor.shape}\")\n",
    "    \n",
    "    return images_tensor, image_paths, labels\n",
    "\n",
    "\n",
    "def get_class_names(data_path: str) -> List[str]:\n",
    "    \"\"\"Retorna lista de nomes das classes\"\"\"\n",
    "    data_path = Path(data_path)\n",
    "    class_folders = sorted([d.name for d in data_path.iterdir() if d.is_dir()])\n",
    "    return class_folders\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings: np.ndarray, \n",
    "                   labels: List[int],\n",
    "                   image_paths: List[str],\n",
    "                   output_path: str):\n",
    "    \"\"\"Salva embeddings e metadados\"\"\"\n",
    "    data = {\n",
    "        'embeddings': embeddings,\n",
    "        'labels': np.array(labels),\n",
    "        'image_paths': image_paths\n",
    "    }\n",
    "    np.savez(output_path, **data)\n",
    "    print(f\"Embeddings salvos em {output_path}\")\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path: str) -> Dict:\n",
    "    \"\"\"Carrega embeddings salvos\"\"\"\n",
    "    data = np.load(embeddings_path, allow_pickle=True)\n",
    "    return {\n",
    "        'embeddings': data['embeddings'],\n",
    "        'labels': data['labels'],\n",
    "        'image_paths': data['image_paths']\n",
    "    }\n",
    "\n",
    "\n",
    "def create_splits(n_samples: int, \n",
    "                 train_ratio: float = 0.7,\n",
    "                 val_ratio: float = 0.15,\n",
    "                 seed: int = 42) -> Tuple[List[int], List[int], List[int]]:\n",
    "    \"\"\"Cria splits train/val/test\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    n_train = int(n_samples * train_ratio)\n",
    "    n_val = int(n_samples * val_ratio)\n",
    "    \n",
    "    train_indices = indices[:n_train]\n",
    "    val_indices = indices[n_train:n_train + n_val]\n",
    "    test_indices = indices[n_train + n_val:]\n",
    "    \n",
    "    return train_indices.tolist(), val_indices.tolist(), test_indices.tolist()\n",
    "\n",
    "\n",
    "def count_parameters(model: torch.nn.Module) -> int:\n",
    "    \"\"\"Conta parâmetros treináveis do modelo\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model: torch.nn.Module,\n",
    "                    images: torch.Tensor,\n",
    "                    batch_size: int = 32,\n",
    "                    device: str = 'cuda') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extrai features das imagens usando o modelo\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo extrator de features\n",
    "        images: Batch de imagens\n",
    "        batch_size: Tamanho do batch\n",
    "        device: Dispositivo\n",
    "    \n",
    "    Returns:\n",
    "        Array numpy de features\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i+batch_size].to(device)\n",
    "        features = model(batch)\n",
    "        all_features.append(features.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(all_features)\n",
    "\n",
    "print(\"Utilitários gerais definidos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. CBIR - Content-Based Image Retrieval <a id=\"4-cbir\"></a>\n",
    "\n",
    "Funções para busca de imagens similares baseada em features visuais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(embeddings: np.ndarray, metric: str = 'cosine', n_neighbors: int = 10):\n",
    "    \"\"\"\n",
    "    Constrói índice de vizinhos mais próximos para CBIR\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Feature embeddings (N x D)\n",
    "        metric: Métrica de distância\n",
    "        n_neighbors: Número de vizinhos\n",
    "    \"\"\"\n",
    "    if metric == 'cosine':\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        metric = 'euclidean'\n",
    "    \n",
    "    nn_model = NearestNeighbors(n_neighbors=n_neighbors, metric=metric, algorithm='auto')\n",
    "    nn_model.fit(embeddings)\n",
    "    \n",
    "    return nn_model\n",
    "\n",
    "\n",
    "def retrieve_similar(query_embedding: np.ndarray,\n",
    "                    nn_model: NearestNeighbors,\n",
    "                    k: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Recupera k imagens mais similares a uma query\"\"\"\n",
    "    if query_embedding.ndim == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "    \n",
    "    distances, indices = nn_model.kneighbors(query_embedding, n_neighbors=k)\n",
    "    return distances[0], indices[0]\n",
    "\n",
    "\n",
    "def batch_retrieve(query_embeddings: np.ndarray,\n",
    "                  nn_model: NearestNeighbors,\n",
    "                  k: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Recupera imagens similares para múltiplas queries\"\"\"\n",
    "    distances, indices = nn_model.kneighbors(query_embeddings, n_neighbors=k)\n",
    "    return distances, indices\n",
    "\n",
    "\n",
    "def compute_recall_at_k(query_labels: np.ndarray,\n",
    "                       retrieved_indices: np.ndarray,\n",
    "                       database_labels: np.ndarray,\n",
    "                       k_values: List[int] = [1, 5, 10]) -> dict:\n",
    "    \"\"\"Calcula Recall@k\"\"\"\n",
    "    results = {}\n",
    "    n_queries = len(query_labels)\n",
    "    \n",
    "    for k in k_values:\n",
    "        correct = 0\n",
    "        for i in range(n_queries):\n",
    "            query_label = query_labels[i]\n",
    "            retrieved_labels = database_labels[retrieved_indices[i, :k]]\n",
    "            if query_label in retrieved_labels:\n",
    "                correct += 1\n",
    "        \n",
    "        recall = correct / n_queries\n",
    "        results[f'Recall@{k}'] = recall\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_precision_at_k(query_labels: np.ndarray,\n",
    "                           retrieved_indices: np.ndarray,\n",
    "                           database_labels: np.ndarray,\n",
    "                           k_values: List[int] = [1, 5, 10]) -> dict:\n",
    "    \"\"\"Calcula Precision@k\"\"\"\n",
    "    results = {}\n",
    "    n_queries = len(query_labels)\n",
    "    \n",
    "    for k in k_values:\n",
    "        total_precision = 0.0\n",
    "        for i in range(n_queries):\n",
    "            query_label = query_labels[i]\n",
    "            retrieved_labels = database_labels[retrieved_indices[i, :k]]\n",
    "            n_relevant = np.sum(retrieved_labels == query_label)\n",
    "            precision = n_relevant / k\n",
    "            total_precision += precision\n",
    "        \n",
    "        avg_precision = total_precision / n_queries\n",
    "        results[f'Precision@{k}'] = avg_precision\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_map_at_k(query_labels: np.ndarray,\n",
    "                    retrieved_indices: np.ndarray,\n",
    "                    database_labels: np.ndarray,\n",
    "                    k: int = 10) -> float:\n",
    "    \"\"\"Calcula Mean Average Precision@k\"\"\"\n",
    "    n_queries = len(query_labels)\n",
    "    average_precisions = []\n",
    "    \n",
    "    for i in range(n_queries):\n",
    "        query_label = query_labels[i]\n",
    "        retrieved_labels = database_labels[retrieved_indices[i, :k]]\n",
    "        \n",
    "        precisions = []\n",
    "        n_relevant = 0\n",
    "        \n",
    "        for j in range(k):\n",
    "            if retrieved_labels[j] == query_label:\n",
    "                n_relevant += 1\n",
    "                precision_at_j = n_relevant / (j + 1)\n",
    "                precisions.append(precision_at_j)\n",
    "        \n",
    "        if len(precisions) > 0:\n",
    "            avg_precision = np.mean(precisions)\n",
    "        else:\n",
    "            avg_precision = 0.0\n",
    "        \n",
    "        average_precisions.append(avg_precision)\n",
    "    \n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "\n",
    "def evaluate_cbir(embeddings: np.ndarray,\n",
    "                 labels: np.ndarray,\n",
    "                 k_values: List[int] = [1, 5, 10],\n",
    "                 metric: str = 'cosine') -> dict:\n",
    "    \"\"\"\n",
    "    Avalia performance de CBIR usando leave-one-out\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Feature embeddings\n",
    "        labels: Labels das classes\n",
    "        k_values: Valores de k para métricas\n",
    "        metric: Métrica de distância\n",
    "    \"\"\"\n",
    "    n_samples = len(embeddings)\n",
    "    max_k = max(k_values) + 1\n",
    "    \n",
    "    if metric == 'cosine':\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        metric = 'euclidean'\n",
    "    \n",
    "    nn_model = NearestNeighbors(n_neighbors=max_k, metric=metric)\n",
    "    nn_model.fit(embeddings)\n",
    "    \n",
    "    _, indices = nn_model.kneighbors(embeddings)\n",
    "    indices = indices[:, 1:]  # Remove a própria query\n",
    "    \n",
    "    results = {}\n",
    "    results.update(compute_recall_at_k(labels, indices, labels, k_values))\n",
    "    results.update(compute_precision_at_k(labels, indices, labels, k_values))\n",
    "    results['MAP@10'] = compute_map_at_k(labels, indices, labels, k=10)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def find_hard_negatives(embeddings: np.ndarray,\n",
    "                       labels: np.ndarray,\n",
    "                       k: int = 10) -> List[Tuple[int, int, float]]:\n",
    "    \"\"\"Encontra pares de hard negatives (imagens similares de classes diferentes)\"\"\"\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    nn_model = NearestNeighbors(n_neighbors=k+1, metric='euclidean')\n",
    "    nn_model.fit(embeddings)\n",
    "    \n",
    "    distances, indices = nn_model.kneighbors(embeddings)\n",
    "    \n",
    "    hard_negatives = []\n",
    "    \n",
    "    for i in range(len(embeddings)):\n",
    "        query_label = labels[i]\n",
    "        for j, idx in enumerate(indices[i, 1:]):\n",
    "            if labels[idx] != query_label:\n",
    "                hard_negatives.append((i, idx, distances[i, j+1]))\n",
    "                break\n",
    "    \n",
    "    return hard_negatives\n",
    "\n",
    "print(\"CBIR definido!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Utilitários de Avaliação <a id=\"5-eval\"></a>\n",
    "\n",
    "Métricas de classificação, visualizações e relatórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn_classifier(train_embeddings: np.ndarray,\n",
    "                        train_labels: np.ndarray,\n",
    "                        n_neighbors: int = 5,\n",
    "                        metric: str = 'cosine') -> KNeighborsClassifier:\n",
    "    \"\"\"Treina classificador KNN\"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "    knn.fit(train_embeddings, train_labels)\n",
    "    return knn\n",
    "\n",
    "\n",
    "def evaluate_knn(knn: KNeighborsClassifier,\n",
    "                test_embeddings: np.ndarray,\n",
    "                test_labels: np.ndarray,\n",
    "                class_names: List[str] = None) -> Dict:\n",
    "    \"\"\"Avalia classificador KNN\"\"\"\n",
    "    predictions = knn.predict(test_embeddings)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(test_labels, predictions),\n",
    "        'macro_f1': f1_score(test_labels, predictions, average='macro'),\n",
    "        'weighted_f1': f1_score(test_labels, predictions, average='weighted'),\n",
    "        'macro_precision': precision_score(test_labels, predictions, average='macro', zero_division=0),\n",
    "        'macro_recall': recall_score(test_labels, predictions, average='macro', zero_division=0),\n",
    "    }\n",
    "    \n",
    "    if class_names is not None:\n",
    "        report = classification_report(test_labels, predictions,\n",
    "                                      target_names=class_names,\n",
    "                                      zero_division=0,\n",
    "                                      output_dict=True)\n",
    "        metrics['classification_report'] = report\n",
    "    \n",
    "    cm = confusion_matrix(test_labels, predictions)\n",
    "    metrics['confusion_matrix'] = cm\n",
    "    metrics['predictions'] = predictions\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray,\n",
    "                         class_names: List[str],\n",
    "                         title: str = 'Confusion Matrix',\n",
    "                         save_path: str = None,\n",
    "                         figsize: tuple = (12, 10)):\n",
    "    \"\"\"Plota matriz de confusão\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Normalized Count'})\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Matriz de confusão salva em {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_tsne(embeddings: np.ndarray,\n",
    "             labels: np.ndarray,\n",
    "             class_names: List[str],\n",
    "             title: str = 't-SNE Visualization',\n",
    "             save_path: str = None,\n",
    "             figsize: tuple = (12, 10),\n",
    "             perplexity: int = 30):\n",
    "    \"\"\"Cria visualização t-SNE dos embeddings\"\"\"\n",
    "    print(\"Computando t-SNE...\")\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        mask = labels == class_idx\n",
    "        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
    "                   label=class_name, alpha=0.6, s=50)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"t-SNE salvo em {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def compute_class_wise_metrics(true_labels: np.ndarray,\n",
    "                               predictions: np.ndarray,\n",
    "                               class_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Calcula métricas por classe\"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    metrics = []\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        true_binary = (true_labels == class_idx).astype(int)\n",
    "        pred_binary = (predictions == class_idx).astype(int)\n",
    "        \n",
    "        tp = np.sum((true_binary == 1) & (pred_binary == 1))\n",
    "        fp = np.sum((true_binary == 0) & (pred_binary == 1))\n",
    "        fn = np.sum((true_binary == 1) & (pred_binary == 0))\n",
    "        tn = np.sum((true_binary == 0) & (pred_binary == 0))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        support = np.sum(true_labels == class_idx)\n",
    "        \n",
    "        metrics.append({\n",
    "            'Class': class_names[class_idx],\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'Support': support\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "def save_evaluation_report(metrics: Dict,\n",
    "                          class_names: List[str],\n",
    "                          save_path: str):\n",
    "    \"\"\"Salva relatório de avaliação\"\"\"\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"SAMI - Evaluation Report\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Overall Metrics:\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Accuracy: {metrics['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Macro F1-Score: {metrics['macro_f1']:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1-Score: {metrics['weighted_f1']:.4f}\\n\")\n",
    "        f.write(f\"Macro Precision: {metrics['macro_precision']:.4f}\\n\")\n",
    "        f.write(f\"Macro Recall: {metrics['macro_recall']:.4f}\\n\\n\")\n",
    "        \n",
    "        if 'classification_report' in metrics:\n",
    "            f.write(\"Per-Class Metrics:\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            report = metrics['classification_report']\n",
    "            \n",
    "            for class_name in class_names:\n",
    "                if class_name in report:\n",
    "                    class_metrics = report[class_name]\n",
    "                    f.write(f\"\\n{class_name}:\\n\")\n",
    "                    f.write(f\"  Precision: {class_metrics['precision']:.4f}\\n\")\n",
    "                    f.write(f\"  Recall: {class_metrics['recall']:.4f}\\n\")\n",
    "                    f.write(f\"  F1-Score: {class_metrics['f1-score']:.4f}\\n\")\n",
    "                    f.write(f\"  Support: {class_metrics['support']}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    print(f\"Relatório salvo em {save_path}\")\n",
    "\n",
    "\n",
    "def compare_k_values(embeddings: np.ndarray,\n",
    "                    labels: np.ndarray,\n",
    "                    k_values: List[int] = [1, 3, 5, 7, 9, 11],\n",
    "                    metric: str = 'cosine') -> pd.DataFrame:\n",
    "    \"\"\"Compara performance do KNN para diferentes valores de k\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        knn.fit(embeddings, labels)\n",
    "        predictions = knn.predict(embeddings)\n",
    "        \n",
    "        acc = accuracy_score(labels, predictions)\n",
    "        f1 = f1_score(labels, predictions, average='macro')\n",
    "        \n",
    "        results.append({'k': k, 'accuracy': acc, 'macro_f1': f1})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Utilitários de avaliação definidos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Clustering Analysis <a id=\"6-clustering\"></a>\n",
    "\n",
    "Descoberta automática de grupos de esponjas sem labels prévios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tsne_clustering(embeddings, perplexity=30, random_state=42):\n",
    "    \"\"\"Computa t-SNE para visualização de clustering\"\"\"\n",
    "    print(f\"Computando t-SNE (perplexity={perplexity})...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)\n",
    "    coords_2d = tsne.fit_transform(embeddings)\n",
    "    return coords_2d\n",
    "\n",
    "\n",
    "def plot_tsne_clusters(coords_2d, labels, title, save_path=None, n_clusters=None):\n",
    "    \"\"\"Plota t-SNE com cores por cluster\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    if n_clusters:\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))\n",
    "        for i in range(n_clusters):\n",
    "            mask = labels == i\n",
    "            plt.scatter(coords_2d[mask, 0], coords_2d[mask, 1], \n",
    "                       c=[colors[i]], label=f'Cluster {i}', \n",
    "                       alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        unique_labels = set(labels)\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for i, color in zip(unique_labels, colors):\n",
    "            if i == -1:\n",
    "                color = 'black'\n",
    "                label = 'Noise'\n",
    "            else:\n",
    "                label = f'Cluster {i}'\n",
    "            \n",
    "            mask = labels == i\n",
    "            plt.scatter(coords_2d[mask, 0], coords_2d[mask, 1],\n",
    "                       c=[color], label=label, alpha=0.6, s=50,\n",
    "                       edgecolors='black', linewidth=0.5)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot salvo em {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def kmeans_clustering(embeddings, n_clusters_range, coords_2d=None, output_dir=None):\n",
    "    \"\"\"Executa K-Means com múltiplos valores de k\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"K-MEANS CLUSTERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for n_clusters in n_clusters_range:\n",
    "        print(f\"\\nTestando k={n_clusters}...\")\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        silhouette = silhouette_score(embeddings, labels)\n",
    "        calinski = calinski_harabasz_score(embeddings, labels)\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "        print(f\"  Calinski-Harabasz: {calinski:.2f}\")\n",
    "        print(f\"  Inertia: {inertia:.2f}\")\n",
    "        \n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"  Amostras por cluster: {dict(zip(unique, counts))}\")\n",
    "        \n",
    "        results.append({\n",
    "            'n_clusters': n_clusters,\n",
    "            'silhouette': silhouette,\n",
    "            'calinski_harabasz': calinski,\n",
    "            'inertia': inertia,\n",
    "            'labels': labels\n",
    "        })\n",
    "        \n",
    "        if coords_2d is not None:\n",
    "            save_path = str(output_dir / f'kmeans_k{n_clusters}.png') if output_dir else None\n",
    "            plot_tsne_clusters(\n",
    "                coords_2d, labels,\n",
    "                f'K-Means (k={n_clusters}) | Silhouette: {silhouette:.3f}',\n",
    "                save_path=save_path,\n",
    "                n_clusters=n_clusters\n",
    "            )\n",
    "    \n",
    "    df = pd.DataFrame([{\n",
    "        'n_clusters': r['n_clusters'],\n",
    "        'silhouette': r['silhouette'],\n",
    "        'calinski_harabasz': r['calinski_harabasz'],\n",
    "        'inertia': r['inertia']\n",
    "    } for r in results])\n",
    "    \n",
    "    best_idx = df['silhouette'].idxmax()\n",
    "    best_k = df.loc[best_idx, 'n_clusters']\n",
    "    print(f\"\\nMelhor k por Silhouette Score: {int(best_k)}\")\n",
    "    \n",
    "    return results, df\n",
    "\n",
    "\n",
    "def dbscan_clustering(embeddings, coords_2d=None, output_dir=None):\n",
    "    \"\"\"Executa DBSCAN (density-based)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DBSCAN CLUSTERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    eps_values = [0.3, 0.5, 0.7, 1.0]\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        print(f\"\\nTestando eps={eps}...\")\n",
    "        \n",
    "        dbscan = DBSCAN(eps=eps, min_samples=5, metric='euclidean')\n",
    "        labels = dbscan.fit_predict(embeddings)\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        print(f\"  Clusters encontrados: {n_clusters}\")\n",
    "        print(f\"  Pontos de ruído: {n_noise}\")\n",
    "        \n",
    "        if n_clusters > 1:\n",
    "            valid_mask = labels != -1\n",
    "            if valid_mask.sum() > 1:\n",
    "                silhouette = silhouette_score(embeddings[valid_mask], labels[valid_mask])\n",
    "                print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "        \n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"  Amostras por cluster: {dict(zip(unique, counts))}\")\n",
    "        \n",
    "        if coords_2d is not None:\n",
    "            save_path = str(output_dir / f'dbscan_eps{eps}.png') if output_dir else None\n",
    "            plot_tsne_clusters(\n",
    "                coords_2d, labels,\n",
    "                f'DBSCAN (eps={eps}) | Clusters: {n_clusters}, Noise: {n_noise}',\n",
    "                save_path=save_path\n",
    "            )\n",
    "\n",
    "\n",
    "def hierarchical_clustering(embeddings, coords_2d=None, output_dir=None):\n",
    "    \"\"\"Executa clustering hierárquico com dendrograma\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHICAL CLUSTERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"Computando matriz de linkage...\")\n",
    "    linkage_matrix = linkage(embeddings, method='ward')\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    dendrogram(linkage_matrix, no_labels=True)\n",
    "    plt.title('Dendrogram - Hierarchical Clustering', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        plt.savefig(output_dir / 'hierarchical_dendrogram.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Dendrograma salvo em {output_dir / 'hierarchical_dendrogram.png'}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    for n in [3, 5, 7, 10]:\n",
    "        clustering = AgglomerativeClustering(n_clusters=n, linkage='ward')\n",
    "        labels = clustering.fit_predict(embeddings)\n",
    "        \n",
    "        silhouette = silhouette_score(embeddings, labels)\n",
    "        print(f\"\\nn_clusters={n}: Silhouette={silhouette:.4f}\")\n",
    "        \n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"  Amostras por cluster: {dict(zip(unique, counts))}\")\n",
    "        \n",
    "        if coords_2d is not None:\n",
    "            save_path = str(output_dir / f'hierarchical_n{n}.png') if output_dir else None\n",
    "            plot_tsne_clusters(\n",
    "                coords_2d, labels,\n",
    "                f'Hierarchical (n={n}) | Silhouette: {silhouette:.3f}',\n",
    "                save_path=save_path,\n",
    "                n_clusters=n\n",
    "            )\n",
    "\n",
    "\n",
    "def save_cluster_examples(image_paths, labels, output_dir, max_per_cluster=20):\n",
    "    \"\"\"Salva imagens exemplo de cada cluster\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SALVANDO EXEMPLOS DOS CLUSTERS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    clusters_dir = Path(output_dir) / 'cluster_images'\n",
    "    clusters_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    unique_labels = sorted(set(labels))\n",
    "    \n",
    "    for cluster_id in unique_labels:\n",
    "        if cluster_id == -1:\n",
    "            cluster_name = 'noise'\n",
    "        else:\n",
    "            cluster_name = f'cluster_{cluster_id}'\n",
    "        \n",
    "        cluster_dir = clusters_dir / cluster_name\n",
    "        cluster_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        cluster_mask = labels == cluster_id\n",
    "        cluster_paths = [image_paths[i] for i in range(len(labels)) if cluster_mask[i]]\n",
    "        \n",
    "        for i, img_path in enumerate(cluster_paths[:max_per_cluster]):\n",
    "            src = Path(img_path)\n",
    "            dst = cluster_dir / f'{i:03d}_{src.name}'\n",
    "            shutil.copy(src, dst)\n",
    "        \n",
    "        print(f\"  {cluster_name}: {len(cluster_paths)} imagens ({min(len(cluster_paths), max_per_cluster)} salvas)\")\n",
    "    \n",
    "    print(f\"\\nExemplos salvos em {clusters_dir}\")\n",
    "\n",
    "print(\"Clustering Analysis definido!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Multi-Scale Patch Clustering <a id=\"7-patches\"></a>\n",
    "\n",
    "Análise de patches multi-escala com UMAP, normalização percentil e remoção de background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_normalize_image(img, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"\n",
    "    Normaliza imagem usando clipping de percentis para melhor contraste.\n",
    "    Aplica normalização por canal para preservar padrões RGB.\n",
    "    \"\"\"\n",
    "    img_normalized = np.zeros_like(img, dtype=np.float32)\n",
    "    \n",
    "    for c in range(3):  # R, G, B\n",
    "        channel = img[:, :, c].astype(np.float32)\n",
    "        \n",
    "        p_low = np.percentile(channel, lower_percentile)\n",
    "        p_high = np.percentile(channel, upper_percentile)\n",
    "        \n",
    "        channel_clipped = np.clip(channel, p_low, p_high)\n",
    "        \n",
    "        if p_high > p_low:\n",
    "            channel_normalized = (channel_clipped - p_low) / (p_high - p_low) * 255.0\n",
    "        else:\n",
    "            channel_normalized = channel_clipped\n",
    "        \n",
    "        img_normalized[:, :, c] = channel_normalized\n",
    "    \n",
    "    return img_normalized.astype(np.uint8)\n",
    "\n",
    "\n",
    "def is_valid_patch(patch, min_content_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Verifica se o patch contém conteúdo suficiente (não é maioritariamente background)\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)\n",
    "    background_mask = (gray > 240) | (gray < 15)\n",
    "    content_ratio = 1.0 - (np.sum(background_mask) / background_mask.size)\n",
    "    return content_ratio >= min_content_ratio\n",
    "\n",
    "\n",
    "def extract_patches_from_image(image_path, window_sizes, stride, max_patches=50, min_content_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Extrai patches de uma imagem com:\n",
    "    - Normalização percentil (5%-95%)\n",
    "    - Remoção de background\n",
    "    - Preservação RGB\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return []\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_normalized = percentile_normalize_image(img_rgb, lower_percentile=5, upper_percentile=95)\n",
    "    \n",
    "    h, w = img_normalized.shape[:2]\n",
    "    patches = []\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        valid_patches_this_size = 0\n",
    "        max_per_size = max_patches // len(window_sizes)\n",
    "        \n",
    "        positions_y = list(range(0, h - window_size + 1, stride))\n",
    "        positions_x = list(range(0, w - window_size + 1, stride))\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        positions = [(y, x) for y in positions_y for x in positions_x]\n",
    "        np.random.shuffle(positions)\n",
    "        \n",
    "        for y, x in positions:\n",
    "            if valid_patches_this_size >= max_per_size:\n",
    "                break\n",
    "            \n",
    "            patch = img_normalized[y:y+window_size, x:x+window_size].copy()\n",
    "            \n",
    "            if not is_valid_patch(patch, min_content_ratio):\n",
    "                continue\n",
    "            \n",
    "            metadata = {\n",
    "                'image_path': str(image_path),\n",
    "                'image_name': Path(image_path).stem,\n",
    "                'window_size': window_size,\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'coords': f'{x}_{y}'\n",
    "            }\n",
    "            \n",
    "            patches.append((patch, metadata))\n",
    "            valid_patches_this_size += 1\n",
    "    \n",
    "    del img, img_rgb, img_normalized\n",
    "    gc.collect()\n",
    "    \n",
    "    return patches\n",
    "\n",
    "\n",
    "def extract_all_patches(image_paths, class_names, window_sizes, stride, max_patches_per_image, min_content_ratio):\n",
    "    \"\"\"Extrai patches de todas as imagens com pré-processamento avançado\"\"\"\n",
    "    all_patches = []\n",
    "    \n",
    "    print(f\"\\nExtraindo patches com:\")\n",
    "    print(f\"  - Normalização percentil (5%-95%)\")\n",
    "    print(f\"  - Remoção de background (min content: {min_content_ratio*100}%)\")\n",
    "    print(f\"  - Preservação de padrões RGB\")\n",
    "    print()\n",
    "    \n",
    "    for img_path, class_name in tqdm(zip(image_paths, class_names), total=len(image_paths)):\n",
    "        patches = extract_patches_from_image(\n",
    "            img_path, window_sizes, stride, max_patches_per_image, min_content_ratio\n",
    "        )\n",
    "        \n",
    "        for patch, metadata in patches:\n",
    "            metadata['class'] = class_name\n",
    "            metadata['prefix'] = f\"{class_name}/{metadata['image_name']}/window_{metadata['window_size']}\"\n",
    "            all_patches.append((patch, metadata))\n",
    "        \n",
    "        if len(all_patches) % 100 == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    print(f\"Total de patches extraídos: {len(all_patches)}\")\n",
    "    \n",
    "    for class_name in set(class_names):\n",
    "        n_patches = sum(1 for _, m in all_patches if m['class'] == class_name)\n",
    "        print(f\"  {class_name}: {n_patches} patches\")\n",
    "    \n",
    "    return all_patches\n",
    "\n",
    "\n",
    "def extract_features_from_patches(patches, model, device, batch_size=32):\n",
    "    \"\"\"Extrai features dos patches preservando informação RGB\"\"\"\n",
    "    print(\"\\nExtraindo features dos patches...\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(patches), batch_size)):\n",
    "        batch_patches = patches[i:i+batch_size]\n",
    "        \n",
    "        batch_tensors = []\n",
    "        for patch_img, _ in batch_patches:\n",
    "            patch_pil = Image.fromarray(patch_img)\n",
    "            patch_tensor = preprocess_image(patch_pil, img_size=224)\n",
    "            batch_tensors.append(patch_tensor)\n",
    "        \n",
    "        batch = torch.stack(batch_tensors).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = model(batch)\n",
    "        \n",
    "        all_features.append(features.cpu().numpy())\n",
    "        \n",
    "        del batch, features\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if i % (batch_size * 10) == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    features_array = np.vstack(all_features)\n",
    "    print(f\"Shape das features: {features_array.shape}\")\n",
    "    \n",
    "    return features_array\n",
    "\n",
    "\n",
    "def cluster_patches(features, n_clusters, metadata_list):\n",
    "    \"\"\"Clusteriza patches\"\"\"\n",
    "    print(f\"\\nClusterizando patches em {n_clusters} grupos...\")\n",
    "    \n",
    "    features_norm = features / (np.linalg.norm(features, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(features_norm)\n",
    "    \n",
    "    silhouette = silhouette_score(features_norm, labels)\n",
    "    print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "    \n",
    "    print(\"\\nEstatísticas dos Clusters:\")\n",
    "    for cluster_id in range(n_clusters):\n",
    "        mask = labels == cluster_id\n",
    "        n_patches = mask.sum()\n",
    "        \n",
    "        classes = [metadata_list[i]['class'] for i in range(len(labels)) if mask[i]]\n",
    "        class_counts = {}\n",
    "        for c in classes:\n",
    "            class_counts[c] = class_counts.get(c, 0) + 1\n",
    "        \n",
    "        print(f\"  Cluster {cluster_id}: {n_patches} patches - {class_counts}\")\n",
    "    \n",
    "    return labels, kmeans, silhouette\n",
    "\n",
    "\n",
    "def visualize_umap_patches(features, labels, metadata_list, save_path=None, silhouette_val=None):\n",
    "    \"\"\"Cria visualização UMAP\"\"\"\n",
    "    if not UMAP_AVAILABLE:\n",
    "        print(\"UMAP não disponível. Pulando visualização UMAP.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nComputando projeção UMAP...\")\n",
    "    \n",
    "    if len(features) > 5000:\n",
    "        print(f\"  Amostrando 5000 patches de {len(features)} para visualização...\")\n",
    "        indices = np.random.choice(len(features), 5000, replace=False)\n",
    "        features_sample = features[indices]\n",
    "        labels_sample = labels[indices]\n",
    "        metadata_sample = [metadata_list[i] for i in indices]\n",
    "    else:\n",
    "        features_sample = features\n",
    "        labels_sample = labels\n",
    "        metadata_sample = metadata_list\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=15,\n",
    "        min_dist=0.1,\n",
    "        n_components=2,\n",
    "        metric='cosine',\n",
    "        random_state=42\n",
    "    )\n",
    "    coords_2d = reducer.fit_transform(features_sample)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Por cluster\n",
    "    ax1 = axes[0]\n",
    "    n_clusters = len(set(labels_sample))\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        mask = labels_sample == cluster_id\n",
    "        if mask.sum() > 0:\n",
    "            ax1.scatter(coords_2d[mask, 0], coords_2d[mask, 1],\n",
    "                       c=[colors[cluster_id]], label=f'Cluster {cluster_id}',\n",
    "                       alpha=0.6, s=20, edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    title = 'UMAP: Por Cluster'\n",
    "    if silhouette_val:\n",
    "        title += f'\\nSilhouette: {silhouette_val:.3f}'\n",
    "    ax1.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('UMAP 1')\n",
    "    ax1.set_ylabel('UMAP 2')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8, ncol=2)\n",
    "    \n",
    "    # Por classe\n",
    "    ax2 = axes[1]\n",
    "    classes = list(set([m['class'] for m in metadata_sample]))\n",
    "    class_colors = plt.cm.Set1(np.linspace(0, 1, len(classes)))\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        mask = np.array([m['class'] == class_name for m in metadata_sample])\n",
    "        if mask.sum() > 0:\n",
    "            ax2.scatter(coords_2d[mask, 0], coords_2d[mask, 1],\n",
    "                       c=[class_colors[i]], label=class_name,\n",
    "                       alpha=0.6, s=20, edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    ax2.set_title('UMAP: Por Classe Original', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('UMAP 1')\n",
    "    ax2.set_ylabel('UMAP 2')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot UMAP salvo em {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def visualize_patch_examples(patches, labels, metadata_list, save_path=None, n_examples=5):\n",
    "    \"\"\"Visualiza patches exemplo de cada cluster\"\"\"\n",
    "    print(\"\\nCriando visualização de patches...\")\n",
    "    \n",
    "    n_clusters = len(set(labels))\n",
    "    \n",
    "    fig, axes = plt.subplots(n_clusters, n_examples, figsize=(n_examples*2, n_clusters*2))\n",
    "    \n",
    "    if n_clusters == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_indices = [i for i, label in enumerate(labels) if label == cluster_id]\n",
    "        \n",
    "        if len(cluster_indices) > n_examples:\n",
    "            sampled_indices = np.random.choice(cluster_indices, n_examples, replace=False)\n",
    "        else:\n",
    "            sampled_indices = cluster_indices[:n_examples]\n",
    "        \n",
    "        for col_idx, patch_idx in enumerate(sampled_indices):\n",
    "            patch_img, metadata = patches[patch_idx]\n",
    "            \n",
    "            axes[cluster_id, col_idx].imshow(patch_img)\n",
    "            axes[cluster_id, col_idx].axis('off')\n",
    "            \n",
    "            if col_idx == 0:\n",
    "                axes[cluster_id, col_idx].set_ylabel(f'Cluster {cluster_id}', \n",
    "                                                     fontsize=10, fontweight='bold')\n",
    "            \n",
    "            class_name = metadata['class']\n",
    "            axes[cluster_id, col_idx].set_title(class_name, fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Patches Exemplo por Cluster', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "        print(f\"Exemplos salvos em {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "print(\"Multi-Scale Patch Clustering definido!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Pipeline de Avaliação <a id=\"8-pipeline\"></a>\n",
    "\n",
    "Pipeline completo para avaliação do modelo SAMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_evaluation(data_path, \n",
    "                        model_path=None,\n",
    "                        model_arch='vit_small',\n",
    "                        output_dir='./results',\n",
    "                        k_neighbors=7,\n",
    "                        batch_size=32,\n",
    "                        img_size=224):\n",
    "    \"\"\"\n",
    "    Pipeline completo de avaliação SAMI\n",
    "    \n",
    "    Args:\n",
    "        data_path: Caminho para o dataset\n",
    "        model_path: Caminho para pesos do modelo (opcional)\n",
    "        model_arch: Arquitetura ('vit_small' ou 'vit_base')\n",
    "        output_dir: Diretório de saída\n",
    "        k_neighbors: k para KNN\n",
    "        batch_size: Tamanho do batch\n",
    "        img_size: Tamanho das imagens\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SAMI - Sponge Automatic Model Identification\")\n",
    "    print(\"Pipeline de Avaliação\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Data path: {data_path}\")\n",
    "    print(f\"Output: {output_dir}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Carregar nomes das classes\n",
    "    class_names = get_class_names(data_path)\n",
    "    print(f\"Classes encontradas ({len(class_names)}):\")\n",
    "    for i, name in enumerate(class_names):\n",
    "        print(f\"  {i}: {name}\")\n",
    "    print()\n",
    "    \n",
    "    # Carregar dataset\n",
    "    print(\"Carregando imagens...\")\n",
    "    images_tensor, image_paths, labels = load_dataset_images(data_path, img_size=img_size)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Carregar modelo\n",
    "    print(f\"\\nCarregando modelo {model_arch}...\")\n",
    "    if model_arch == 'vit_small':\n",
    "        model = vit_small(patch_size=16)\n",
    "    elif model_arch == 'vit_base':\n",
    "        model = vit_base(patch_size=16)\n",
    "    \n",
    "    if model_path:\n",
    "        print(f\"Carregando pesos de {model_path}\")\n",
    "        state_dict = torch.load(model_path, map_location='cpu')\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print(\"AVISO: Usando inicialização aleatória (sem pesos pré-treinados)\")\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(DEVICE)\n",
    "    print(f\"Modelo carregado em {DEVICE}\")\n",
    "    print(f\"Parâmetros: {count_parameters(model):,}\\n\")\n",
    "    \n",
    "    # Extrair features\n",
    "    print(\"Extraindo features...\")\n",
    "    embeddings = extract_features(model, images_tensor, batch_size=batch_size, device=DEVICE)\n",
    "    print(f\"Shape dos embeddings: {embeddings.shape}\\n\")\n",
    "    \n",
    "    # Avaliação 1: CBIR\n",
    "    print(\"=\"*80)\n",
    "    print(\"AVALIAÇÃO 1: Content-Based Image Retrieval (CBIR)\")\n",
    "    print(\"=\"*80)\n",
    "    cbir_results = evaluate_cbir(embeddings, labels, k_values=[1, 5, 10, 20], metric='cosine')\n",
    "    print(\"Resultados CBIR:\")\n",
    "    for metric, value in cbir_results.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Avaliação 2: KNN\n",
    "    print(\"=\"*80)\n",
    "    print(\"AVALIAÇÃO 2: K-Nearest Neighbors Classification\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"Treinando KNN (k={k_neighbors})...\")\n",
    "    knn = train_knn_classifier(embeddings, labels, n_neighbors=k_neighbors, metric='cosine')\n",
    "    \n",
    "    print(\"Avaliando KNN...\")\n",
    "    knn_metrics = evaluate_knn(knn, embeddings, labels, class_names=class_names)\n",
    "    \n",
    "    print(\"\\nResultados KNN:\")\n",
    "    print(f\"  Accuracy: {knn_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Macro F1: {knn_metrics['macro_f1']:.4f}\")\n",
    "    print(f\"  Weighted F1: {knn_metrics['weighted_f1']:.4f}\")\n",
    "    print(f\"  Macro Precision: {knn_metrics['macro_precision']:.4f}\")\n",
    "    print(f\"  Macro Recall: {knn_metrics['macro_recall']:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Avaliação 3: Métricas por classe\n",
    "    print(\"=\"*80)\n",
    "    print(\"AVALIAÇÃO 3: Métricas por Classe\")\n",
    "    print(\"=\"*80)\n",
    "    class_metrics = compute_class_wise_metrics(labels, knn_metrics['predictions'], class_names)\n",
    "    print(class_metrics.to_string(index=False))\n",
    "    class_metrics.to_csv(output_dir / 'class_metrics.csv', index=False)\n",
    "    print(f\"\\nMétricas salvas em {output_dir / 'class_metrics.csv'}\\n\")\n",
    "    \n",
    "    # Avaliação 4: Comparação de valores de k\n",
    "    print(\"=\"*80)\n",
    "    print(\"AVALIAÇÃO 4: Comparação de Valores de K\")\n",
    "    print(\"=\"*80)\n",
    "    k_comparison = compare_k_values(embeddings, labels, k_values=[1, 3, 5, 7, 9, 11, 15], metric='cosine')\n",
    "    print(k_comparison.to_string(index=False))\n",
    "    k_comparison.to_csv(output_dir / 'k_comparison.csv', index=False)\n",
    "    print(f\"\\nComparação salva em {output_dir / 'k_comparison.csv'}\\n\")\n",
    "    \n",
    "    # Visualizações\n",
    "    print(\"=\"*80)\n",
    "    print(\"GERANDO VISUALIZAÇÕES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nMatriz de Confusão...\")\n",
    "    plot_confusion_matrix(\n",
    "        knn_metrics['confusion_matrix'],\n",
    "        class_names,\n",
    "        title=f'SAMI Confusion Matrix (k={k_neighbors})',\n",
    "        save_path=str(output_dir / 'confusion_matrix.png')\n",
    "    )\n",
    "    \n",
    "    print(\"\\nt-SNE...\")\n",
    "    plot_tsne(\n",
    "        embeddings,\n",
    "        labels,\n",
    "        class_names,\n",
    "        title='SAMI t-SNE Embedding Visualization',\n",
    "        save_path=str(output_dir / 't-sne_visualization.png'),\n",
    "        perplexity=min(30, len(embeddings) // 5)\n",
    "    )\n",
    "    \n",
    "    # Salvar relatório\n",
    "    print(\"\\nSalvando relatório...\")\n",
    "    all_metrics = {**knn_metrics, 'cbir': cbir_results}\n",
    "    save_evaluation_report(all_metrics, class_names, str(output_dir / 'evaluation_report.txt'))\n",
    "    \n",
    "    # Salvar embeddings\n",
    "    save_embeddings(embeddings, labels.tolist(), image_paths, str(output_dir / 'embeddings.npz'))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AVALIAÇÃO COMPLETA!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nResultados salvos em: {output_dir}\")\n",
    "    print(\"\\nArquivos gerados:\")\n",
    "    print(\"  - class_metrics.csv\")\n",
    "    print(\"  - k_comparison.csv\")\n",
    "    print(\"  - confusion_matrix.png\")\n",
    "    print(\"  - t-sne_visualization.png\")\n",
    "    print(\"  - evaluation_report.txt\")\n",
    "    print(\"  - embeddings.npz\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'embeddings': embeddings,\n",
    "        'labels': labels,\n",
    "        'knn_metrics': knn_metrics,\n",
    "        'cbir_results': cbir_results,\n",
    "        'class_metrics': class_metrics,\n",
    "        'k_comparison': k_comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Exemplo de Uso <a id=\"9-example\"></a>\n",
    "\n",
    "Demonstração básica do uso do SAMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXEMPLO 1: Avaliação Completa\n",
    "# ============================================================================\n",
    "# Descomente e configure os caminhos para executar\n",
    "\n",
    "# results = run_full_evaluation(\n",
    "#     data_path='./imagefolder_cambrian_sponges',\n",
    "#     model_path=None,  # ou './path/to/weights.pth'\n",
    "#     model_arch='vit_small',\n",
    "#     output_dir='./results',\n",
    "#     k_neighbors=7,\n",
    "#     batch_size=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXEMPLO 2: Clustering Analysis\n",
    "# ============================================================================\n",
    "# Descomente e configure para executar análise de clustering\n",
    "\n",
    "# data_path = './imagefolder_cambrian_sponges'\n",
    "# output_dir = Path('./clustering_results')\n",
    "# output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# # Carregar dados\n",
    "# images_tensor, image_paths, labels = load_dataset_images(data_path, img_size=224)\n",
    "\n",
    "# # Carregar modelo\n",
    "# model = vit_small(patch_size=16)\n",
    "# model.eval()\n",
    "# model = model.to(DEVICE)\n",
    "\n",
    "# # Extrair features\n",
    "# embeddings = extract_features(model, images_tensor, batch_size=32, device=DEVICE)\n",
    "# embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# # t-SNE\n",
    "# coords_2d = compute_tsne_clustering(embeddings, perplexity=30)\n",
    "\n",
    "# # K-Means\n",
    "# results, df = kmeans_clustering(embeddings, [3, 5, 7, 10], coords_2d, output_dir)\n",
    "\n",
    "# # DBSCAN\n",
    "# dbscan_clustering(embeddings, coords_2d, output_dir)\n",
    "\n",
    "# # Hierarchical\n",
    "# hierarchical_clustering(embeddings, coords_2d, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXEMPLO 3: Multi-Scale Patch Clustering\n",
    "# ============================================================================\n",
    "# Descomente e configure para análise de patches\n",
    "\n",
    "# data_path = Path('./imagefolder_cambrian_sponges')\n",
    "# output_dir = Path('./patch_clustering_results')\n",
    "# output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# window_sizes = [64, 128, 256]\n",
    "# stride = 32\n",
    "# max_patches_per_image = 50\n",
    "# min_content_ratio = 0.7\n",
    "# n_clusters = 10\n",
    "\n",
    "# # Listar imagens\n",
    "# class_folders = [d for d in data_path.iterdir() if d.is_dir()]\n",
    "# image_paths = []\n",
    "# class_names = []\n",
    "# for class_folder in class_folders:\n",
    "#     files = list(class_folder.glob('*.jpg')) + list(class_folder.glob('*.png'))\n",
    "#     for f in files:\n",
    "#         image_paths.append(f)\n",
    "#         class_names.append(class_folder.name)\n",
    "\n",
    "# # Extrair patches\n",
    "# all_patches = extract_all_patches(\n",
    "#     image_paths, class_names, window_sizes, \n",
    "#     stride, max_patches_per_image, min_content_ratio\n",
    "# )\n",
    "\n",
    "# # Modelo\n",
    "# model = vit_small(patch_size=16)\n",
    "# model.eval()\n",
    "# model = model.to(DEVICE)\n",
    "\n",
    "# # Features\n",
    "# features = extract_features_from_patches(all_patches, model, DEVICE, batch_size=32)\n",
    "# metadata_list = [m for _, m in all_patches]\n",
    "\n",
    "# # Clustering\n",
    "# labels, kmeans, silhouette = cluster_patches(features, n_clusters, metadata_list)\n",
    "\n",
    "# # Visualização UMAP\n",
    "# visualize_umap_patches(features, labels, metadata_list, \n",
    "#                        save_path=str(output_dir / 'umap.png'),\n",
    "#                        silhouette_val=silhouette)\n",
    "\n",
    "# # Exemplos de patches\n",
    "# visualize_patch_examples(all_patches, labels, metadata_list, \n",
    "#                          save_path=str(output_dir / 'patch_examples.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXEMPLO 4: Inferência em Imagem Individual\n",
    "# ============================================================================\n",
    "\n",
    "def predict_single_image(image_path, model, embeddings_db, labels_db, class_names, k=5):\n",
    "    \"\"\"\n",
    "    Prediz a classe de uma imagem individual usando CBIR\n",
    "    \n",
    "    Args:\n",
    "        image_path: Caminho para a imagem\n",
    "        model: Modelo ViT\n",
    "        embeddings_db: Embeddings do banco de dados\n",
    "        labels_db: Labels do banco de dados\n",
    "        class_names: Nomes das classes\n",
    "        k: Número de vizinhos\n",
    "    \"\"\"\n",
    "    # Carregar e preprocessar\n",
    "    img = load_image(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Erro ao carregar {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    img_tensor = preprocess_image(img, img_size=224)\n",
    "    img_batch = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # Extrair features\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        query_features = model(img_batch).cpu().numpy()\n",
    "    \n",
    "    # Normalizar\n",
    "    query_features = query_features / np.linalg.norm(query_features, axis=1, keepdims=True)\n",
    "    embeddings_norm = embeddings_db / np.linalg.norm(embeddings_db, axis=1, keepdims=True)\n",
    "    \n",
    "    # Encontrar vizinhos\n",
    "    nn_model = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "    nn_model.fit(embeddings_norm)\n",
    "    distances, indices = nn_model.kneighbors(query_features)\n",
    "    \n",
    "    # Votação\n",
    "    neighbor_labels = labels_db[indices[0]]\n",
    "    unique, counts = np.unique(neighbor_labels, return_counts=True)\n",
    "    predicted_class = unique[np.argmax(counts)]\n",
    "    \n",
    "    print(f\"\\nResultados para: {image_path}\")\n",
    "    print(f\"Classe predita: {class_names[predicted_class]}\")\n",
    "    print(f\"\\nTop {k} vizinhos mais próximos:\")\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        print(f\"  {i+1}. {class_names[labels_db[idx]]} (distância: {dist:.4f})\")\n",
    "    \n",
    "    return predicted_class, distances, indices\n",
    "\n",
    "\n",
    "# Exemplo de uso:\n",
    "# predicted, dists, idxs = predict_single_image(\n",
    "#     './test_image.jpg',\n",
    "#     model,\n",
    "#     embeddings,\n",
    "#     labels,\n",
    "#     class_names,\n",
    "#     k=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Estrutura de Diretórios Esperada\n",
    "\n",
    "```\n",
    "imagefolder_cambrian_sponges/\n",
    "├── Archaeocyatha_sp1/\n",
    "│   ├── specimen_001.jpg\n",
    "│   ├── specimen_002.jpg\n",
    "│   └── ...\n",
    "├── Porifera_sp2/\n",
    "│   └── ...\n",
    "└── ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notas\n",
    "\n",
    "- Para usar pesos pré-treinados, forneça o caminho em `model_path`\n",
    "- UMAP requer `pip install umap-learn`\n",
    "- Para datasets grandes, ajuste `batch_size` conforme memória disponível\n",
    "- O Silhouette Score varia de -1 a 1, com >0.5 indicando boa separação de clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_minor": "0",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
